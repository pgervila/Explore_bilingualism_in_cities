{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### IMPORT LIBRARIES AND TWITTER API SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Stream, OAuthHandler, StreamListener\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import pyprind\n",
    "import deepdish as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import secret codes\n",
    "from twitter_pwd import access_token, access_token_secret, consumer_key, consumer_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEY FUNCTIONS : USERS, FOLLOWERS, TIMELINES, LANGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_account_network(account_name, rel_type='followers', max_num =100, key_words=None, \n",
    "                        min_num_tweets=0, min_num_followers=0):\n",
    "    \"\"\" Given an account by account_name, \n",
    "        find all users that are linked to it via a specified relation type 'rel_type'.\n",
    "        Args:\n",
    "            * account_name: string. Twitter account name\n",
    "            * rel_type: string. Specifies relation type (default is 'followers')\n",
    "            * max_num: integer. Maximum number of 'related' users considered\n",
    "            * key_words: list of strings. Used to filter retrieved users by location,\n",
    "                if specified\n",
    "            * min_num_tweets: minimum number of tweets a follower needs to have \n",
    "                to be included in list\n",
    "            * min_num_followers: minimum number of followers a follower needs to have \n",
    "                to be included in list\n",
    "        Returns:\n",
    "            * list_people: list of account_names\n",
    "    \"\"\"\n",
    "    pbar = pyprind.ProgBar(max_num)\n",
    "    list_people = []\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    # limit max number of followers requested to max exisitng followers\n",
    "#     num_acc_followers = api.get_user(account_name)\n",
    "#     if  num_acc_followers <= max_num:\n",
    "#         max_num = num_acc_followers\n",
    "        \n",
    "    # very important to set count=200 MAX VALUE -> Max 3000 accounts per 15 minutes interval\n",
    "    users = tweepy.Cursor(getattr(api, rel_type, 0), \n",
    "                          screen_name=account_name, count=200).items(max_num)\n",
    "    while True:\n",
    "        try:\n",
    "            user = next(users)\n",
    "            if not key_words:\n",
    "                if user.statuses_count > min_num_tweets:\n",
    "                    list_people.append(user._json)\n",
    "            else:\n",
    "                locs = '|'.join(key_words)\n",
    "                patt = re.compile(locs)\n",
    "                found_loc = re.findall(patt, user._json['location'])\n",
    "                if found_loc and user.statuses_count > min_num_tweets:\n",
    "                    list_people.append(user._json)\n",
    "        except tweepy.TweepError as e:\n",
    "            if 'Read timed out' in str(e):\n",
    "                print('fallen here')\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                time.sleep(60*16)\n",
    "                user = next(users)\n",
    "        except StopIteration:\n",
    "            break            \n",
    "        pbar.update()\n",
    "    return list_people\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_account_tweets(account_name, max_num_twts=20):\n",
    "    \"\"\" Given an account name,\n",
    "        it retrieves a maximum number of tweets written or retweeted by account owner.\n",
    "        It returns them in a list.\n",
    "        Args:\n",
    "            * account name: string. Screen_name that identifies the twitter account\n",
    "            * max_num_twts: integer. Maximum number of tweets to be retrieved for each account\n",
    "        Returns:\n",
    "            * list_tweets: list including info of all retrieved tweets in JSON format\"\"\"\n",
    "    list_tweets=[]\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    timeline = tweepy.Cursor(api.user_timeline, screen_name=account_name, \n",
    "                             count=200, include_rts = True).items(max_num_twts)\n",
    "    i=0\n",
    "    while True:\n",
    "        try:\n",
    "            tw = next(timeline)\n",
    "            list_tweets.append(tw)\n",
    "        except tweepy.TweepError as e:\n",
    "            if '401' in str(e):    \n",
    "                print(e)\n",
    "                time.sleep(10)\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(60*15)\n",
    "                tw = next(timeline)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return list_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_from_accounts(list_accounts, max_num_accounts=None, max_num_twts=20):\n",
    "    \"\"\" Given a list of accounts, get tweets texts, langs and authors\n",
    "        All URLs and tweet account names are removed from tweet\n",
    "        texts since they are not relevant for language identification\"\"\"\n",
    "    pbar = pyprind.ProgBar(len(list_accounts))\n",
    "    texts_tweets = []\n",
    "    langs_tweets = []\n",
    "    authors_tweets = []\n",
    "    if max_num_accounts:\n",
    "        list_accounts = list_accounts[:max_num_accounts]\n",
    "    for idx, acc in enumerate(list_accounts):\n",
    "        twts = get_account_tweets(acc, max_num_twts=max_num_twts)\n",
    "        texts_tweets.extend([re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", tw.text) \n",
    "                             for tw in twts])\n",
    "        langs_tweets.extend([tw.lang for tw in twts])\n",
    "        authors_tweets.extend([acc for _ in twts])\n",
    "        #[detect(txt) for txt in texts_tweets if re.sub(\" \",\"\", txt)])\n",
    "        pbar.update()\n",
    "    return texts_tweets, langs_tweets, authors_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tweets_from_followers(screen_name, country, node_name, city=None, \n",
    "                               min_num_twts_per_acc=5, max_num_followers=None):\n",
    "    \"\"\" Creates pandas dataframe with all tweets texts \n",
    "        from followers of a given account and corresponding language. \n",
    "        A dataframe with all followers info must have been previously computed \n",
    "        and saved in hdf5 format\n",
    "        Args:\n",
    "            * screen_name:\n",
    "            * country:\n",
    "            * node_name:\n",
    "    \"\"\"\n",
    "    base_path = '/'.join(['',country, node_name, screen_name])\n",
    "    path_load = base_path + '/followers'\n",
    "    path_save = base_path + '/tls_followers'\n",
    "    key_words = {'ukr':{'all':r\"(Україна|Ukraine|Украина|Київ|Киев|Kiev|Kyiv|Львів|Львов|Одес)\", \n",
    "                        'Kiev':r\"(Kiev|Kyiv|Київ|Киев)\"}, \n",
    "                 'cat':{'all':r\"(Barcel|Catal|Tarr|Llei|Ger|Gir|Badal|Sabad|Terrass)\",\n",
    "                        'Terrassa':r\"(Terras|Vall)\",\n",
    "                        'Girona':r\"(Giro|Gero)\",\n",
    "                        'Vic':r\"(Vic|Oson)\", 'Barcelona':r\"(Barcel|barcel|Bcn|bcn)\",\n",
    "                        'Tarragona':r\"(Tarrag|tarrag)\", 'Lleida':r\"(Lleida|Lerid|Lérid)\",\n",
    "                        'Badalona':r\"Badal\"}}\n",
    "    df = pd.read_hdf('lang_data.h5', path_load)\n",
    "    # filter by num_min_twts_per_account\n",
    "    relevant_followers = df['screen_name'][df['statuses_count'] >= min_num_twts_per_acc]\n",
    "    # keep only country residents\n",
    "    if city:\n",
    "        relevant_followers = relevant_followers[df['location'].str.contains(key_words[country][city])].values\n",
    "    else:\n",
    "        relevant_followers = relevant_followers[df['location'].str.contains(key_words[country]['all'])].values\n",
    "        \n",
    "    texts, langs, auth = get_tweets_from_accounts(relevant_followers, \n",
    "                                                  max_num_accounts=max_num_followers)\n",
    "    df_txts_langs= pd.DataFrame({'texts':texts, 'lang':langs, 'screen_name':auth})\n",
    "    df_txts_langs.to_hdf('lang_data.h5', path_save)\n",
    "    return df_txts_langs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "class CityTweets:\n",
    "    key_words = {'ukr':{'all':r\"(Україна|Ukraine|Украина|Київ|Киев|Kiev|Kyiv|Львів|Львов|Одес)\", \n",
    "                    'Kiev':r\"(Kiev|Kyiv|Київ|Киев)\"}, \n",
    "                 'cat':{'all':r\"(Barcel|Catal|Tarr|Llei|Ger|Gir|Badal)\",\n",
    "                        'Terrassa':r\"(Terras|Vall)\",\n",
    "                        'Girona':r\"(Giro|Gero)\",\n",
    "                        'Vic':r\"(Vic|Oson)\", 'Barcelona':r\"(Barcel|barcel|Bcn|bcn)\",\n",
    "                        'Tarragona':r\"(Tarrag|tarrag)\", 'Lleida':r\"(Lleida|Lerid|Lérid)\",\n",
    "                        'Badalona':r\"Badal\"}}\n",
    "    def __init__(self, account_name, min_num_tweets=0, \n",
    "                 min_num_followers=0, key_words=None):\n",
    "        self.account_name = account_name\n",
    "        self.min_num_tweets = min_num_tweets\n",
    "        self.min_num_followers = min_num_followers\n",
    "        self.key_words = key_words\n",
    "        \n",
    "        self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        \n",
    "    def get_account_network(rel_type='followers', max_num =100, key_words=None, \n",
    "                            min_num_tweets=0, min_num_followers=0):\n",
    "        \"\"\" Given an account by account_name, \n",
    "            find all users that are linked to it via a specified relation type 'rel_type'.\n",
    "            Args:\n",
    "                * rel_type: string. Specifies relation type (default is 'followers')\n",
    "                * max_num: integer. Maximum number of 'related' users considered\n",
    "                * key_words: list of strings. Used to filter retrieved users by location,\n",
    "                    if specified\n",
    "                * min_num_tweets: minimum number of tweets a follower needs to have \n",
    "                    to be included in list\n",
    "                * min_num_followers: minimum number of followers a follower needs to have \n",
    "                    to be included in list\n",
    "            Returns:\n",
    "                * list_people: list of account_names\n",
    "        \"\"\"\n",
    "        pbar = pyprind.ProgBar(max_num)\n",
    "        self.list_people = []\n",
    "        \n",
    "        # very important to set count=200 MAX VALUE\n",
    "        users = tweepy.Cursor(getattr(self.api, rel_type, 0), screen_name=self.account_name, \n",
    "                              count=200).items(max_num)\n",
    "        while True:\n",
    "            try:\n",
    "                user = next(users)\n",
    "                if not key_words:\n",
    "                    if user.statuses_count > min_num_tweets and user.followers_count > min_num_followers:\n",
    "                        self.list_people.append(user._json)\n",
    "                else:\n",
    "                    locs = '|'.join(key_words)\n",
    "                    patt = re.compile(locs)\n",
    "                    found_loc = re.findall(patt, user._json['location'], flags=re.I)\n",
    "                    if found_loc and user.statuses_count > min_num_tweets and user.followers_count > min_num_followers:\n",
    "                        self.list_people.append(user._json)\n",
    "            except tweepy.TweepError as e:\n",
    "                if 'Read timed out' in str(e):\n",
    "                    print('fallen here')\n",
    "                    print(e)\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    time.sleep(60*16)\n",
    "                    user = next(users)\n",
    "            except StopIteration:\n",
    "                break            \n",
    "            pbar.update()\n",
    "    \n",
    "    def get_account_tweets(max_num_twts=20):\n",
    "        \"\"\" Given an account name,\n",
    "            it retrieves a maximum number of tweets written or retweeted by account owner.\n",
    "            It returns them in a list.\n",
    "            Args:\n",
    "                * account name: string. Screen_name that identifies the twitter account\n",
    "                * max_num_twts: integer. Maximum number of tweets to be retrieved for each account\n",
    "            Returns:\n",
    "                * list_tweets: list including info of all retrieved tweets in JSON format\n",
    "        \"\"\"\n",
    "        list_tweets=[]\n",
    "        timeline = tweepy.Cursor(self.api.user_timeline, screen_name=self.account_name, \n",
    "                                 count=200, include_rts = True).items(max_num_twts)\n",
    "        i=0\n",
    "        while True:\n",
    "            try:\n",
    "                tw = next(timeline)\n",
    "                list_tweets.append(tw)\n",
    "            except tweepy.TweepError as e:\n",
    "                if '401' in str(e):    \n",
    "                    print(e)\n",
    "                    time.sleep(10)\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(60*15)\n",
    "                    tw = next(timeline)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return list_tweets\n",
    "    \n",
    "    def get_tweets_from_accounts(max_num_accounts=None, max_num_twts=20):\n",
    "        \"\"\" Given a list of accounts, get tweets texts, langs and authors\n",
    "            All URLs and tweet account names are removed from tweet\n",
    "            texts since they are not relevant for language identification\n",
    "        \"\"\"\n",
    "        pbar = pyprind.ProgBar(len(list_accounts))\n",
    "        texts_tweets = []\n",
    "        langs_tweets = []\n",
    "        authors_tweets = []\n",
    "        if max_num_accounts:\n",
    "            list_accounts = self.list_people[:max_num_accounts]\n",
    "        for idx, acc in enumerate(list_accounts):\n",
    "            twts = self.get_account_tweets(acc, max_num_twts=max_num_twts)\n",
    "            texts_tweets.extend([re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", tw.text) \n",
    "                                 for tw in twts])\n",
    "            langs_tweets.extend([tw.lang for tw in twts])\n",
    "            authors_tweets.extend([acc for _ in twts])\n",
    "            pbar.update()\n",
    "        return texts_tweets, langs_tweets, authors_tweets\n",
    "\n",
    "    \n",
    "    def save_tweets_from_followers(screen_name, country, node_name, city=None, \n",
    "                                   min_num_twts_per_acc=10, max_num_followers=None):\n",
    "        \"\"\" Creates pandas dataframe with all tweets texts \n",
    "            from followers of a given account and corresponding language. \n",
    "            A dataframe with all followers info must have been previously computed \n",
    "            and saved in hdf5 format\n",
    "            Args:\n",
    "                * screen_name:\n",
    "                * country:\n",
    "                * node_name:\n",
    "        \"\"\"\n",
    "        base_path = '/'.join(['',country, node_name, screen_name])\n",
    "        path_load = base_path + '/followers'\n",
    "        path_save = base_path + '/tls_followers'\n",
    "\n",
    "        df = pd.read_hdf('lang_data.h5', path_load)\n",
    "        # filter by num_min_twts_per_account\n",
    "        relevant_followers = df['screen_name'][df['statuses_count'] >= min_num_twts_per_acc]\n",
    "        # keep only country residents\n",
    "        if city:\n",
    "            relevant_followers = relevant_followers[df['location'].str.contains(self.key_words[country][city])].values\n",
    "        else:\n",
    "            relevant_followers = relevant_followers[df['location'].str.contains(self.key_words[country]['all'])].values\n",
    "\n",
    "        texts, langs, auth = self.get_tweets_from_accounts(max_num_accounts=max_num_followers)\n",
    "        df_txts_langs= pd.DataFrame({'texts':texts, 'lang':langs, 'screen_name':auth})\n",
    "        df_txts_langs.to_hdf('lang_data.h5', path_save)\n",
    "        return df_txts_langs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['киев']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytxt = \"киев\"\n",
    "re.findall(r\"Київ|Киев\", mytxt, flags=re.I )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UKRAINE: data structure and relevant twitter accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ukraine_nodes = {}\n",
    "Ukraine_nodes['cities'] = ['kiev', 'odessa', 'lviv', 'kharkov', 'dnipropetrovsk']\n",
    "Ukraine_nodes['city_sites'] = {'Mariupol':['0629ComUa'], \n",
    "                               'kiev':['nashkiev', 'kievtypical', \n",
    "                                       'kliniki_kiev', 'LISOD_clinic','avto_kiev', \n",
    "                                       'editbeauty']}\n",
    "Ukraine_nodes['news'] = ['HromadskeUA', 'tsnua', 'ukrpravda_news', 'lb_ua', 'Korrespondent', \n",
    "                         'Delo_ua', 'BBC_ua', 'LIGAnet', 'segodnya_life']\n",
    "Ukraine_nodes['TV'] = ['5channel', 'EspresoTV', '24tvua', 'footballua_tv']\n",
    "Ukraine_nodes['starsystem'] = ['VeraBrezhneva', 's_vakarchuk', 'KAMEHCKUX']\n",
    "Ukraine_nodes['politics'] = ['poroshenko', 'Vitaliy_Klychko', \n",
    "                             'Leshchenkos', 'AvakovArsen', 'andriy_sadovyi', 'GennadyKernes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_words=['Україна', 'Ukraine', 'Украина', 'Київ', 'Киев']\n",
    "HromadskeUA_followers = get_account_network('HromadskeUA', rel_type='followers', \n",
    "                                            max_num =5000, key_words=key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country = 'ukr'\n",
    "node_name = 'politics'\n",
    "acc_name = 'poroshenko'\n",
    "rel_type = 'followers'\n",
    "\n",
    "#key_words=['Україна', 'Ukraine', 'Украина', 'Київ', 'Киев']\n",
    "path_save = '/'.join(['',country, node_name, acc_name, rel_type])\n",
    "followers = get_account_network(acc_name, rel_type=rel_type, \n",
    "                                max_num =5000, key_words=None)\n",
    "json_format = [elem._json for elem in followers]\n",
    "df = pd.DataFrame(json_format)\n",
    "df.to_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>Україна Львів</td>\n",
       "      <td>juliaskab13</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>Львов</td>\n",
       "      <td>novinska_cat</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>Львов</td>\n",
       "      <td>HumenuykRoma</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>Львів</td>\n",
       "      <td>kaprikorn87</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Україна Львів</td>\n",
       "      <td>Hollywell83</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803</th>\n",
       "      <td>м .Львів</td>\n",
       "      <td>M8tgs4YGZ391fNw</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>Львів</td>\n",
       "      <td>cherniak_gi</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           location      screen_name lang\n",
       "522   Україна Львів      juliaskab13   uk\n",
       "1006          Львов     novinska_cat   ru\n",
       "1661          Львов     HumenuykRoma   ru\n",
       "2269          Львів      kaprikorn87   uk\n",
       "2997  Україна Львів      Hollywell83   uk\n",
       "3803       м .Львів  M8tgs4YGZ391fNw   uk\n",
       "4731          Львів      cherniak_gi   uk"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['location','screen_name','lang']][df['location'].str.contains(r\"(Львів|Львов)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uk    8\n",
       "ru    3\n",
       "en    1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lang'][df['location'].str.contains(r\"(Львів|Львов|Lviv|Івано-Франківськ)\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ru    14\n",
       "en     2\n",
       "uk     1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lang'][df['statuses_count'] >= 10][df['location'].str.contains(r\"(Kiev|Kyiv|Київ|Киев)\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['lang','screen_name', 'followers_count']][df['statuses_count'] >= 200][df['location'].str.contains(r\"(Kiev|Kyiv|Київ|Киев)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HRMUA_flwrs = [f for idx, f in df_HRMUA.iterrows()]\n",
    "#HRMUA_texts, HRMUA_langs = get_tweets_from_accounts(HRMUA_flwrs, max_num_followers=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_txts_langs_HRMUA = pd.DataFrame({'texts':HRMUA_texts, 'lang':HRMUA_langs})\n",
    "\n",
    "df_txts_langs_HRMUA.to_hdf('lang_data.h5', '/ukr_nodes/news/HromadskeUA/tls_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_txts = pd.read_hdf('lang_data.h5', '/ukr_nodes/news/HromadskeUA/tls_followers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATALONIA NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Catalonia_nodes = {}\n",
    "Catalonia_nodes['news'] = ['LaVanguardia', 'VilaWeb', 'diariARA', 'elperiodico',\n",
    "                           'elperiodico_cat', 'elpuntavui']\n",
    "Catalonia_nodes['cities'] = ['bcn_ajuntament', 'paerialleida', 'girona_cat', 'TGNAjuntament', \n",
    "                             'AjBadalona', 'aj_vic', 'ajterrassa']\n",
    "Catalonia_nodes['politics'] = ['KRLS', 'junqueras', 'AdaColau', \n",
    "                              'miqueliceta', 'InesArrimadas', 'Albiol_XG', \n",
    "                              'raulromeva', 'ForcadellCarme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_account_followers(country, node, acc_name):\n",
    "    path_save = '/'.join(['', country, node, acc_name, 'followers'])\n",
    "    followers = get_account_network(acc_name, rel_type='followers', max_num =5000)\n",
    "    json_format = [elem._json for elem in followers]\n",
    "    df = pd.DataFrame(json_format)\n",
    "    df.to_hdf('lang_data.h5', path_save)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = get_account_followers(country ='cat', node='cities', acc_name='AjBadalona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es       3586\n",
       "ca       1102\n",
       "en        271\n",
       "fr          9\n",
       "it          7\n",
       "en-gb       5\n",
       "pt          4\n",
       "de          3\n",
       "eu          2\n",
       "ar          2\n",
       "ru          2\n",
       "gl          1\n",
       "pl          1\n",
       "en-GB       1\n",
       "zh-cn       1\n",
       "ja          1\n",
       "tr          1\n",
       "ro          1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n",
      "//anaconda/lib/python3.5/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "es    688\n",
       "ca    281\n",
       "en     51\n",
       "fr      1\n",
       "it      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r\"(Badal)\"\n",
    "df_Bad = df[df['statuses_count'] >= 10][df['location'].str.contains(regex)]\n",
    "df_Bad['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE TWEETS FROM FOLLOWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_txts_langs = save_tweets_from_followers('AjBadalona' , 'cat', 'cities', city='Badalona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_txts_langs['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LANG DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lang', 'screen_name', 'texts'], dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txts_langs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_txts_langs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d955a0e55e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_txts_langs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lang'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_txts_langs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'und'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_txts_langs' is not defined"
     ]
    }
   ],
   "source": [
    "df_txts_langs[['texts', 'lang', 'screen_name']][df_txts_langs['lang'] == 'und'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/'.join(['',country, node_name, screen_name])\n",
    "path_load = base_path + '/followers'\n",
    "path_save = base_path + '/tls_followers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "# filter followers to focus on most relevant ones\n",
    "min_num_twts = 5\n",
    "relevant_followers = df['screen_name'][df['statuses_count'] >= min_num_twts][\n",
    "                         df['location'].str.contains(key_words[country])\n",
    "                       ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_txts_langs['lang'].value_counts()\n",
    "#langs_detected = [detect(txt) for txt in df_txts_langs['texts']]\n",
    "\n",
    "langs_detected=[]\n",
    "for txt in df_txts_langs['texts']:\n",
    "    try:\n",
    "        langs_detected.append(detect(txt))\n",
    "    except:\n",
    "        langs_detected.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_try = pd.DataFrame({'a':['aaaa','bbbfdde',1],'b':[23,44,56]})\n",
    "df_try2 = pd.DataFrame({'a':['xxxx','zzzz'],'b':[3233,43214]})\n",
    "\n",
    "store = pd.HDFStore('try_hyerar.h5')\n",
    "\n",
    "store.append('city/topic', df_try)\n",
    "\n",
    "store.close()\n",
    "\n",
    "pd.read_hdf('try_hyerar.h5', 'city/topic')\n",
    "\n",
    "store = pd.HDFStore('try_hyerar.h5','a')\n",
    "\n",
    "store.append('city/topic', df_try2)\n",
    "\n",
    "store.close()\n",
    "\n",
    "pd.read_hdf('try_hyerar.h5', 'city/topic')\n",
    "\n",
    "store = pd.HDFStore('try_hyerar.h5','a')\n",
    "\n",
    "store.put('city/followers',df)\n",
    "\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### EXPLORE AN ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Украина, Киев'"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = tweepy.API(auth)\n",
    "user_info = api.get_user('kliniki_kiev')  #ArnauAndreu\n",
    "user_info._json['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2287"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info.followers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get timeline\n",
    "tl = get_account_tweets('onlyforulonely',max_num_twts=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' frefe tt  ewdwed    oo'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"@dijdoer frefe tt http://ewdowide.ewd.ewde ewdwed @jeiwo @ pgvila http://ewdowide.ewd.ewde oo\"\n",
    "re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[(elem._json['lang'],elem._json['text']) for elem in tl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uk'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Ну нічого-нічого, скоро і моя інста стане популярною і всі будуть такі мол огоо Ладка яка ти класна, а я така оо дякую цьом-цьом.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user_info._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_friends = get_account_network('ArnauAndreu')\n",
    "\n",
    "my_df = pd.DataFrame(my_followers)\n",
    "\n",
    "Counter([friend.lang for friend in my_friends])\n",
    "\n",
    "my_fr_txts, my_friends_lang = get_tweets_from_accounts(my_friends)\n",
    "\n",
    "#Counter(my_friends_lang)\n",
    "\n",
    "\n",
    "\n",
    "# my_fr_langs_detected=[]\n",
    "# for i,txt in enumerate(my_fr_txts):\n",
    "#     #print(i, txt)\n",
    "#     try:\n",
    "#         my_fr_langs_detected.append(detect(txt))\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "#Counter(my_fr_langs_detected)\n",
    "\n",
    "usr_tl = get_account_tweets(my_friends[44].screen_name, max_num=10)\n",
    "\n",
    "df_try=pd.DataFrame([twt._json for twt in usr_tl])\n",
    "df_try.columns\n",
    "\n",
    "my_tl = get_account_tweets('ArnauAndreu', max_num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get hdf database keys\n",
    "with pd.HDFStore('lang_data.h5') as f:\n",
    "    my_keys = f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'lang_data.h5'\n",
    "country = 'cat'\n",
    "acc_names = ['diariARA', 'LaVanguardia']\n",
    "load_node1 = '/' + country + '_nodes/news/' + acc_names[0] + '/tls_followers'\n",
    "load_node2 = '/' + country + '_nodes/news/' + acc_names[1] + '/tls_followers'\n",
    "\n",
    "df1 = pd.read_hdf(file_path, load_node1)\n",
    "df2 = pd.read_hdf(file_path, load_node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df1, df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44937, 2), (34461, 2), (12181, 2))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape, df1.shape, df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TWITTER RANDOM WALK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish city's lingua franca out of random inhabitants that are also Twitter users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Make a set with 5-10 relevant inhabitants or accounts, with lots of followers and tweets. Start from mayor, city hall account, local newspapers, police, traffic or any other relevant account based in the city.\n",
    "\n",
    "2. Get a follower from the city (or county, oblast) as first node ( Check this follower \n",
    "   has sufficient tweets and followers)\n",
    "\n",
    "3. Get follower of step2 node\n",
    "\n",
    "4. Repeat step3 with new nodes until max number of nodes is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria to keep tweets from user:\n",
    "1. User ratio num_tweets/num_followers < 30\n",
    "2. User ratio num_friends/num_followers < 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### UPDATE ALGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Update all acc networks(s1) with 'get_main_unique_followers' and s2 = saved_ones (get them from unique saved tweet authors) main_unique_followers \n",
    "\n",
    "2. Check s1.difference(s2)\n",
    "\n",
    "3. new_twts = RWCT.get_account_tweets(new_unique_follws)\n",
    "\n",
    "4. old_twts = RWCT.load_main_unique_followers() and old_twts.append(new_twts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Barcelona/tweets_from_followers', '/Barcelona/unique_followers', '/Kiev/tweets_from_followers', '/Kiev/unique_followers', '/Kiev/5channel/followers', '/Kiev/Gordonuacom/followers', '/Kiev/HromadskeUA/followers', '/Kiev/KyivOperativ/followers', '/Kiev/VWK668/followers', '/Kiev/Vitaliy_Klychko/followers', '/Kiev/auto_kiev/followers', '/Kiev/kievtypical/followers', '/Kiev/kyivmetroalerts/followers', '/Kiev/patrolpoliceua/followers', '/Kiev/poroshenko/followers', '/Kiev/tsnua/followers', '/Kiev/ukrpravda_news/followers', '/Barcelona/LaVanguardia/followers', '/Barcelona/TMB_Barcelona/followers', '/Barcelona/bcn_ajuntament/followers', '/Barcelona/diariARA/followers', '/Barcelona/meteocat/followers', '/Barcelona/mossos/followers']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore('city_random_walks.h5') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RW_tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RW_tweets.py\n",
    "import os\n",
    "import tweepy\n",
    "from tweepy import Stream, OAuthHandler, StreamListener\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import pyprind\n",
    "import deepdish as dd\n",
    "\n",
    "#import secret codes\n",
    "from twitter_pwd import access_token, access_token_secret, consumer_key, consumer_secret\n",
    "\n",
    "class RandomWalkCityTweets:\n",
    "    \n",
    "    \"\"\" Get tweets from random relevant followers that live in a given city\n",
    "        and return data on language use \"\"\"\n",
    "    \n",
    "    Kiev_dict = {'KyivOperativ', 'kyivmetroalerts', 'nashkiev', 'auto_kiev', \n",
    "             'Leshchenkos', 'poroshenko', 'Vitaliy_Klychko', 'kievtypical', \n",
    "             'ukrpravda_news', 'HromadskeUA','lb_ua', 'Korrespondent', \n",
    "             'LIGAnet', 'radiosvoboda', '5channel', 'tsnua', 'VWK668', 'Gordonuacom', 'zn_ua',\n",
    "             'patrolpoliceua', 'KievRestaurants'}\n",
    "    \n",
    "    Barcelona_dict = {'TMB_Barcelona', 'bcn_ajuntament', 'barcelona_cat', 'LaVanguardia', 'VilaWeb', \n",
    "                  'diariARA', 'elperiodico', 'elperiodico_cat', 'elpuntavui', 'meteocat', 'mossos'}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, data_file_name, city):\n",
    "        if not os.path.exists(data_file_name):\n",
    "            open(data_file_name, 'w+').close()\n",
    "        self.data_file_name = data_file_name\n",
    "        self.city = city\n",
    "        self.key_words = {'Barcelona':{'country': ['Catalu'], \n",
    "                                       'city': ['Barcel']}, \n",
    "                          'Kiev':{'country': ['Україна', 'Ukraine', 'Украина'], \n",
    "                                  'city': ['Kiev' ,'Kyiv' , 'Київ' , 'Киев']}}\n",
    "        \n",
    "        #set_up API\n",
    "        auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        \n",
    "            \n",
    "    def get_account_network(self, account_name, rel_type='followers', max_num =100,\n",
    "                            min_num_tweets=0, min_num_followers=0, only_city=False, \n",
    "                            limited_search=False, avoid_repeat=None, cursor_on=False):\n",
    "        \"\"\" Given an account by account_name, \n",
    "            find all users that are linked to it via a specified relation type 'rel_type'.\n",
    "            Args:\n",
    "                * account_name: string. Twitter account name\n",
    "                * rel_type: string. Specifies relation type (default is 'followers')\n",
    "                * max_num: integer. Maximum number of 'related' users considered\n",
    "                * key_words: list of strings. Used to filter retrieved users by location,\n",
    "                    if specified\n",
    "                * min_num_tweets: minimum number of tweets a follower needs to have \n",
    "                    to be included in list\n",
    "                * min_num_followers: minimum number of followers a follower needs to have \n",
    "                    to be included in list\n",
    "            Returns:\n",
    "                * list_people: list of account_names\n",
    "        \"\"\"\n",
    "        pbar = pyprind.ProgBar(max_num)\n",
    "        list_people = []\n",
    "        # very important to set count=200 MAX VALUE -> Max 3000 accounts per 15 minutes interval\n",
    "        if not cursor_on:\n",
    "            cursor = tweepy.Cursor(getattr(self.api, rel_type, 0), \n",
    "                                   screen_name=account_name, count=200)\n",
    "        else:\n",
    "            node = '/'.join(['', self.city, account_name, rel_type])\n",
    "            df_old = pd.read_hdf(self.data_file_name, node)\n",
    "            cursor_id = df_old.cursor_id.values[-1]\n",
    "            cursor = tweepy.Cursor(api.followers, screen_name=account_name, \n",
    "                                      count=200, cursor=cursor_id)\n",
    "        users = cursor.items(max_num)\n",
    "        while True:\n",
    "            try:\n",
    "                user = next(users)\n",
    "                if only_city:\n",
    "                    locs = '|'.join(self.key_words[self.city]['city'])\n",
    "                else:\n",
    "                    locs = '|'.join(self.key_words[self.city]['country'] + \n",
    "                                    self.key_words[self.city]['city'])                   \n",
    "                patt = re.compile(locs)\n",
    "                found_loc = re.findall(patt, user._json['location'])\n",
    "                if (found_loc and user.statuses_count > min_num_tweets and \n",
    "                    not user.protected):\n",
    "                    user._json.update({'cursor_id': cursor.iterator.next_cursor})\n",
    "                    if avoid_repeat:\n",
    "                        if user._json['screen_name'] not in avoid_repeat:\n",
    "                            list_people.append(user._json)\n",
    "                    else:\n",
    "                        list_people.append(user._json)\n",
    "                    if len(list_people) > 2 and limited_search:\n",
    "                        break\n",
    "            except tweepy.TweepError as e:\n",
    "                if 'Read timed out' in str(e):\n",
    "                    print('fallen here')\n",
    "                    print(e)\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    time.sleep(60*16)\n",
    "                    continue #user = next(users)\n",
    "            except StopIteration:\n",
    "                break            \n",
    "            pbar.update()\n",
    "        if not limited_search:\n",
    "            node = '/'.join(['', self.city, account_name, rel_type])\n",
    "            df_new = pd.DataFrame(list_people)\n",
    "            if not cursor_on:\n",
    "                with pd.HDFStore(self.data_file_name) as f:\n",
    "                    if node not in f.keys():\n",
    "                        df_new.to_hdf(self.data_file_name, node)\n",
    "                        # update list unique followers\n",
    "            else:\n",
    "                df_old = df_old.append(df_new, ignore_index=True)\n",
    "                df_old.to_hdf(self.data_file_name, node)\n",
    "                \n",
    "        return list_people\n",
    "    \n",
    "    def get_main_unique_followers(self, save=True, min_num_flws_per_acc=50): \n",
    "        \"\"\" Read all followers nodes and compute list of \n",
    "            all unique followers for a given city\n",
    "        \"\"\"\n",
    "        key_words='|'.join(self.key_words[self.city]['city'])\n",
    "        #initialize frame\n",
    "        df_unique_flws = pd.DataFrame()\n",
    "        with pd.HDFStore(self.data_file_name) as f:\n",
    "            for n in f.keys():\n",
    "                if re.findall(self.city, n) and not re.findall('_followers', n):\n",
    "                    df = pd.read_hdf(f, n)\n",
    "                    df = df[(df['followers_count'] > min_num_flws_per_acc) & \n",
    "                            (df['statuses_count'] / df['followers_count'] < 30) &\n",
    "                            (df['location'].str.contains(key_words))]\n",
    "                    df_unique_flws = df_unique_flws.append(df)\n",
    "            self.df_unique_flws = df_unique_flws.drop_duplicates('screen_name')\n",
    "            if save:\n",
    "                self.df_unique_flws.to_hdf(self.data_file_name, \n",
    "                                           '/'.join(['', self.city, 'unique_followers']))\n",
    "                \n",
    "    def load_main_unique_followers(self):\n",
    "        \"\"\" Load main followers from hdf file and ssign them to class attribute \n",
    "            as pandas Dataframe\"\"\"\n",
    "        self.df_unique_flws = pd.read_hdf(self.data_file_name, \n",
    "                                          '/'.join(['', self.city, 'unique_followers']))\n",
    "           \n",
    "    def get_account_tweets(self, account_name, max_num_twts=20):\n",
    "        \"\"\" Given an account name,\n",
    "            it retrieves a maximum number of tweets written or retweeted by account owner.\n",
    "            It returns them in a list.\n",
    "            Args:\n",
    "                * account name: string. Screen_name that identifies the twitter account\n",
    "                * max_num_twts: integer. Maximum number of tweets to be retrieved for each account\n",
    "            Returns:\n",
    "                * list_tweets: list including info of all retrieved tweets in JSON format\"\"\"\n",
    "        list_tweets = []\n",
    "        timeline = tweepy.Cursor(self.api.user_timeline, screen_name=account_name, \n",
    "                                 count=200, include_rts = True).items(max_num_twts)\n",
    "        while True:\n",
    "            try:\n",
    "                tw = next(timeline)\n",
    "                list_tweets.append(tw)\n",
    "            except tweepy.TweepError as e:\n",
    "                if '401' in str(e):    \n",
    "                    print(e)\n",
    "                    time.sleep(3)\n",
    "                    break\n",
    "                elif '404' in str(e):\n",
    "                    print(e)\n",
    "                    time.sleep(3)\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(60*15)\n",
    "                    continue \n",
    "            except StopIteration:\n",
    "                break\n",
    "        return list_tweets\n",
    "\n",
    "    def get_tweets_from_accounts(self, list_accounts, max_num_accounts=None, \n",
    "                                 max_num_twts=20, save=True, random_walk=False):\n",
    "        \"\"\" Given a list of accounts, get tweets texts, langs and authors\n",
    "            All URLs and tweet account names are removed from tweet\n",
    "            texts since they are not relevant for language identification\n",
    "        \"\"\"\n",
    "        pbar = pyprind.ProgBar(len(list_accounts))\n",
    "        texts_tweets = []\n",
    "        langs_tweets = []\n",
    "        authors_tweets = []\n",
    "        if max_num_accounts:\n",
    "            list_accounts = list_accounts[:max_num_accounts]\n",
    "        for idx, acc in enumerate(list_accounts):\n",
    "            twts = self.get_account_tweets(acc, max_num_twts=max_num_twts)\n",
    "            texts_tweets.extend([re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", tw.text) \n",
    "                                 for tw in twts])\n",
    "            langs_tweets.extend([tw.lang for tw in twts])\n",
    "            authors_tweets.extend([acc for _ in twts])\n",
    "            pbar.update()\n",
    "        df_tweets = pd.DataFrame({'tweets':texts_tweets, \n",
    "                                  'lang':langs_tweets, \n",
    "                                  'screen_name':authors_tweets})\n",
    "        df_tweets = df_tweets.drop_duplicates('tweets')\n",
    "        if save:\n",
    "            if not random_walk:\n",
    "                df_tweets.to_hdf(self.data_file_name, \n",
    "                                 '/'.join(['', self.city, 'tweets_from_followers']))\n",
    "            else:\n",
    "                with pd.HDFStore('city_random_walks.h5') as f:\n",
    "                    nodes = f.keys()\n",
    "                digits = []\n",
    "                pattern = r\"\".join([self.city, \"/random_walk_\", \"(\\d+)\"])\n",
    "                for e in nodes:\n",
    "                    try:\n",
    "                        digits.append(int(re.findall(pattern, e)[0]))\n",
    "                    except:\n",
    "                        continue\n",
    "                if not digits:\n",
    "                    df_tweets.to_hdf(self.data_file_name, \n",
    "                                     '/'.join(['', self.city, 'random_walk_1']))\n",
    "                else:\n",
    "                    i = max(digits) \n",
    "                    df_tweets.to_hdf(self.data_file_name, \n",
    "                                     '/'.join(['', self.city, 'random_walk_' + str(i + 1)]))               \n",
    "        return df_tweets\n",
    "    \n",
    "    def update_tweets_from_main_followers(self, download=False):\n",
    "        \"\"\" Download tweets from newly detected followers and append them to saved data\"\"\"\n",
    "        # get sets\n",
    "        self.get_main_unique_followers(save=True)\n",
    "        all_flws = set(self.df_unique_flws)\n",
    "        availab_tweets = pd.read_hdf(self.data_file_name, \n",
    "                                        '/'.join(['', self.city, 'tweets_from_followers']))\n",
    "        flws_with_twts = set(availab_tweets['screen_name'])\n",
    "        # compute set difference\n",
    "        new_flws = all_flws.difference(flws_with_twts)\n",
    "        # get tweets from new followers if any\n",
    "        if new_flws:\n",
    "            new_twts = self.get_account_tweets(new_flws, save=False)\n",
    "            # append new tweets\n",
    "            availab_tweets = availab_tweets.append(new_twts, ignore_index=True)\n",
    "            # save\n",
    "            availab_tweets.to_hdf('city_random_walks.h5', \n",
    "                                  '/'.join(['', self.city, 'tweets_from_followers']))\n",
    "            \n",
    "    def random_walk(self):\n",
    "        \"\"\" \n",
    "            Select a list of accounts by randomly walking \n",
    "            all main followers' friends and followers\n",
    "        \"\"\"        \n",
    "        \n",
    "        # load main followers\n",
    "        self.load_main_unique_followers()\n",
    "        \n",
    "        # get random sample from main followers\n",
    "        sample = np.random.choice(self.df_unique_flws['screen_name'], 10, replace=False)\n",
    "        \n",
    "        # get a random follower and friend from each account from sample \n",
    "        # ( check they do not belong to already met accounts and main followers !!)\n",
    "        all_flws = []\n",
    "        for acc in sample:\n",
    "            # look for friend and follower\n",
    "            list_flws = self.get_account_network(acc, min_num_tweets=10,\n",
    "                                                 only_city=True,\n",
    "                                                 limited_search=True, avoid_repeat=all_flws)\n",
    "            all_flws.extend(list_flws)\n",
    "            list_friends = self.get_account_network(acc, min_num_tweets=10, \n",
    "                                                    rel_type='friends', only_city=True, \n",
    "                                                    limited_search=True, avoid_repeat=all_flws)\n",
    "            all_flws.extend(list_flws)\n",
    "        self.random_walk_accounts = pd.DataFrame(all_flws)\n",
    "        print('starting to retrieve tweets')\n",
    "        self.random_walk_tweets = self.get_tweets_from_accounts(self.random_walk_accounts[\"screen_name\"],  \n",
    "                                                                max_num_twts=20, save=True, \n",
    "                                                                random_walk=True)\n",
    "            \n",
    "#     def random_walk(self, regex_words = None):\n",
    "#         \"\"\" \n",
    "#             Select a list of accounts by randomly walking \n",
    "#             all main followers' friends and followers\n",
    "#         \"\"\"\n",
    "#         main_flws = []\n",
    "#         with pd.HDFStore(self.data_file_name) as f:\n",
    "#             for n in f.keys():\n",
    "#                 if re.findall( r\"followers\", n):\n",
    "#                     flws = pd.read_hdf(self.data_file_name, n)\n",
    "#                     #####\n",
    "#                     idx = [i for i, w in enumerate(key_words) \n",
    "#                            if re.findall(city, w)][0]\n",
    "#                     regex_ words  =\"|\".join(key_words[idx:])\n",
    "#                     try:\n",
    "#                         min_people = 50\n",
    "#                         flws = flws[\n",
    "#                                     (flws['friends_count'] > 50) & \n",
    "#                                     (flws['followers_count'] > 50) &\n",
    "#                                     (flws['location'].str.contains(regex_ words)) &\n",
    "#                                     (flws['lang'].isin(['ru', 'uk']))\n",
    "#                                    ]\n",
    "#                     except \n",
    "#                     flws = flws['screen_name'].sample(50).unique().tolist()\n",
    "#                     main_flws.extend(flws)\n",
    "#         main_flws = set(main_flws)\n",
    "        \n",
    "#         sub_flws = []\n",
    "#         for acc in main_flws:\n",
    "#             for rel in ['friends', 'followers']:\n",
    "#                 sub_flws.extend(self.get_account_network(\n",
    "#                         acc, rel_type=rel, max_num=3000, \n",
    "#                         regex_ words=key_words)\n",
    "#                                )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[###                           ] | ETA: 00:00:490%                          100%\n",
      "[##                            ] | ETA: 00:00:250%                          100%\n",
      "[#####                         ] | ETA: 00:00:110%                          100%\n",
      "[###########                   ] | ETA: 00:00:040%                          100%\n",
      "[######                        ] | ETA: 00:00:130%                          100%\n",
      "[###########                   ] | ETA: 00:00:040%                          100%\n",
      "[###                           ] | ETA: 00:00:190%                          100%\n",
      "[                              ]0%                          100%\n",
      "[######                        ] | ETA: 00:00:050%                          100%\n",
      "[#######                       ] | ETA: 00:00:130%                          100%\n",
      "[###                           ] | ETA: 00:00:210%                          100%\n",
      "[###                           ] | ETA: 00:00:280%                          100%\n",
      "[#######                       ] | ETA: 00:00:040%                          100%\n",
      "[###                           ] | ETA: 00:00:120%                          100%\n",
      "[####                          ] | ETA: 00:00:100%                          100%\n",
      "[####                          ] | ETA: 00:00:120%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n",
      "0%                          100%\n",
      "[##                            ] | ETA: 00:00:330%                          100%\n",
      "[#                             ] | ETA: 00:01:580%                          100%\n",
      "[##                            ] | ETA: 00:00:490%                          100%\n",
      "[##############################] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to retrieve tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:02:28\n"
     ]
    }
   ],
   "source": [
    "Bcn_rw = RandomWalkCityTweets('city_random_walks.h5', 'Barcelona')\n",
    "Bcn_rw.random_walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56,)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bcn_rw.random_walk_accounts[\"screen_name\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528,)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bcn_rw.random_walk_tweets['lang'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kiev_rw = RandomWalkCityTweets('city_random_walks.h5', 'Kiev')\n",
    "#kiev_rw.get_main_unique_followers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kiev_rw.random_walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ru    14\n",
       "uk     8\n",
       "en     2\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiev_rw.random_walk_accounts.drop_duplicates('screen_name')['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ru     230\n",
       "uk      92\n",
       "und     39\n",
       "en      15\n",
       "cs       4\n",
       "bg       3\n",
       "ro       1\n",
       "de       1\n",
       "sl       1\n",
       "lt       1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiev_rw.random_walk_tweets['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kiev_rw.random_walk_accounts[['description', 'screen_name', 'lang', 'id']].drop_duplicates('screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kiev_rw.df_unique_flws.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kiev_rw.load_main_unique_followers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1123,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiev_rw.df_unique_flws['screen_name'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set(pd.read_hdf('city_random_walks.h5', '/Kiev/patrolpoliceua/followers')['screen_name'])\n",
    "s2 = set(kiev_rw.df_unique_flws['screen_name'])\n",
    "len(s1.intersection(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#twts = kiev_rw.get_tweets_from_accounts(kiev_rw.df_unique_flws['screen_name'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bcn_rw = RandomWalkCityTweets('city_random_walks.h5', 'Barcelona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bcn_rw.get_account_network('mossos', max_num =36000, min_num_tweets=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bcn_rw.df_unique_flws['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#twts['lang'].value_counts()\n",
    "#twts['screen_name'][twts['lang'] == 'en'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TWEET LANG COUNTS PER USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twt_from_flws_Kiev = pd.read_hdf('city_random_walks.h5', \n",
    "                                 '/Kiev/tweets_from_followers')\n",
    "\n",
    "twt_from_flws_Kiev.pivot_table(aggfunc={'lang':'count'},\n",
    "                               index='screen_name', \n",
    "                               columns='lang').fillna(0.).lang[['ru', 'uk', 'en', 'und']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(lang\n",
       " ru    9.701839\n",
       " uk    4.922556\n",
       " en    2.820910\n",
       " dtype: float64, lang\n",
       " ru    6.536730\n",
       " uk    5.819791\n",
       " en    4.911017\n",
       " dtype: float64, lang\n",
       " ru    11.0\n",
       " uk     2.0\n",
       " en     0.0\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_counts_per_acc = twt_from_flws_Kiev.groupby(['screen_name', 'lang']).count().unstack()['tweets'][['ru', 'uk', 'en']].fillna(0.)\n",
    "\n",
    "lang_counts_per_acc.mean(), lang_counts_per_acc.std(), lang_counts_per_acc.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(706, 3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_counts_per_acc[lang_counts_per_acc['ru'] > lang_counts_per_acc['uk']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c1c34a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals_ru = lang_counts_per_acc['ru'].values\n",
    "vals_uk = lang_counts_per_acc['uk'].values\n",
    "#np.histogram(vals_ru)\n",
    "alpha = 0.5\n",
    "plt.hist(vals_ru, bins=20, color='red', label='ru', alpha=alpha, edgecolor='black', normed=True)\n",
    "plt.hist(vals_uk, bins=20, color='blue', label='uk', alpha=alpha, edgecolor='black', normed=True)\n",
    "plt.xticks(range(21))\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bcn_lang_per_acc = pd.read_hdf('city_random_walks.h5', \n",
    "                               '/Barcelona/tweets_from_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115007, 4)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bcn_lang_per_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es           55787\n",
       "ca           29047\n",
       "en           15529\n",
       "Undefined     7166\n",
       "pt            2228\n",
       "Name: lang_detected, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bcn_lang_per_acc['lang_detected'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(lang_detected\n",
       " ca           3.782573\n",
       " es           7.853094\n",
       " en           2.235505\n",
       " Undefined    0.984039\n",
       " dtype: float64, lang_detected\n",
       " ca           4.276879\n",
       " es           5.468378\n",
       " en           3.851483\n",
       " Undefined    1.386437\n",
       " dtype: float64, lang_detected\n",
       " ca           2.0\n",
       " es           7.0\n",
       " en           1.0\n",
       " Undefined    1.0\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_counts_per_acc_Bcn = Bcn_lang_per_acc.groupby(['screen_name', 'lang_detected']).count().unstack()['tweets'].fillna(0)[['ca', 'es', 'en', 'Undefined']]\n",
    "lang_counts_per_acc_Bcn.mean(), lang_counts_per_acc_Bcn.std(), lang_counts_per_acc_Bcn.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([452, 445, 453, 400, 334, 381, 324, 323, 335, 317, 303, 305, 271,\n",
       "        307, 290, 259, 201, 182, 133, 125]),\n",
       " array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(vals_es, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cda0dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals_es = lang_counts_per_acc_Bcn['es'].values\n",
    "vals_ca = lang_counts_per_acc_Bcn['ca'].values\n",
    "vals_en = lang_counts_per_acc_Bcn['en'].values\n",
    "np.histogram(vals_es)\n",
    "plt.hist(vals_es, bins=20, color='r', label='es', alpha=0.7, edgecolor='black', normed=True)\n",
    "plt.hist(vals_ca, bins=20, color='y', label='ca', alpha=0.7, edgecolor='black', normed=True)\n",
    "plt.xticks(range(21))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kiev_dict = {'KyivOperativ', 'kyivmetroalerts', 'nashkiev', 'auto_kiev', \n",
    "             'Leshchenkos', 'poroshenko', 'Vitaliy_Klychko', 'kievtypical', \n",
    "             'ukrpravda_news', 'HromadskeUA','lb_ua', 'Korrespondent', \n",
    "             'LIGAnet', 'radiosvoboda', '5channel', 'tsnua', 'VWK668', 'Gordonuacom', 'zn_ua',\n",
    "             'patrolpoliceua'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Barcelona_dict = {'TMB_Barcelona', 'bcn_ajuntament', 'barcelona_cat', 'LaVanguardia', 'VilaWeb', \n",
    "                  'diariARA', 'elperiodico', 'elperiodico_cat', 'elpuntavui', 'meteocat', 'mossos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node1 = 'Kiev/KyivOperativ/followers'\n",
    "node2 = 'Kiev/auto_kiev/followers'\n",
    "\n",
    "Ky_Op = pd.read_hdf('city_random_walks.h5', node1)\n",
    "Au_Ki = pd.read_hdf('city_random_walks.h5', node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ky_Op['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(674, 1826, 371)"
      ]
     },
     "execution_count": 1109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set(Ky_Op['screen_name'].tolist())\n",
    "s2 = set(Au_Ki['screen_name'].tolist())\n",
    "s3 = s1.intersection(s2)\n",
    "len(s1), len(s2), len(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 2)"
      ]
     },
     "execution_count": 1119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ky_Op[Ky_Op['screen_name'].isin(s3)][['followers_count', 'screen_name']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1274\n",
       "False     552\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Au_Ki['statuses_count'] / Au_Ki['followers_count'] < 30).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Au_Ki[['followers_count', 'screen_name', 'statuses_count']][Au_Ki['followers_count'] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node = 'Barcelona/meteocat/followers'\n",
    "ww = pd.read_hdf('city_random_walks.h5', node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2373, 47)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ww.keys()\n",
    "ww.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww[['lang', 'description', \n",
    "    'screen_name', 'location']][(ww['followers_count'] > 50) &\n",
    "                                (ww['location'].str.contains(r\"Kiev|Kyiv|Київ|Киев\"))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es       1603\n",
       "ca        465\n",
       "en        251\n",
       "fr          6\n",
       "en-gb       5\n",
       "it          5\n",
       "pt          3\n",
       "de          2\n",
       "en-GB       1\n",
       "gl          1\n",
       "zh-CN       1\n",
       "ja          1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww['lang'][(ww['followers_count'] > 40) & \n",
    "           (ww['location'].str.contains(r\"Barc\"))].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ww[['lang', 'description', 'screen_name']][(ww['followers_count'] > 20) & \n",
    "#                (ww['location'].str.contains(r\"Kiev|Kyiv|Київ|Киев\")) &\n",
    "#                                           (ww['lang'].isin(['ru', 'uk']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ww['description'][ww['description'].str.len() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txts = [re.sub(r\"[^\\w\\s]+\", \"\", desc, flags=re.U) for desc in descriptions ]\n",
    "lgs = []\n",
    "for ix, txt in enumerate(txts):\n",
    "    try:\n",
    "        counts = Counter([detect(txt) for _ in range(50)])\n",
    "        lgs.append([ix, counts])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('dummy_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':np.random.randint(1,10,10), 'b':np.random.randint(1,10,10)})\n",
    "df.to_hdf('dummy_dir/newHDF5.h5', 'aa/bb/cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imp = pd.read_hdf('dummy_dir/newHDF5.h5', 'aa/bb/cc')\n",
    "#df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oo = pd.read_hdf('city_random_walks.h5', '/Kiev/kyivmetroalerts/followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ru       155\n",
       "uk        57\n",
       "en        41\n",
       "fr         1\n",
       "en-gb      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo['lang'][oo['location'].str.contains(r\"Kiev|Kyiv|Київ|Киев\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rrr = pd.read_hdf('city_random_walks.h5', '/Kiev/tweets_from_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twts.to_hdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[(acc['screen_name'], acc['lang'], acc['location'])  for acc in acc_ntw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "follws = [(acc.statuses_count, acc.lang, acc.screen_name, acc.followers_count, acc.friends_count, acc.location) \n",
    "                 for acc in acc_ntw if re.findall(r\"Kiev|Kyiv|Киї|Киев\", acc.location) and \n",
    "                 acc.statuses_count > 50 and acc.lang in ['ru', 'uk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb=list(zip(*follws))\n",
    "col_keys = ['statuses_count', \"lang\", 'screen_name', 'followers_count', 'friends_count', 'location']\n",
    "d_bb = {key:val for key,val in zip(col_keys,bb)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(d_bb).sort_values(by=\"statuses_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomWalkCityTweets:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kl_follwrs = pd.read_hdf('lang_data.h5', '/ukr/politics/Vitaliy_Klychko/followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kl_follwrs[['screen_name', 'followers_count']][(Kl_follwrs['statuses_count'] > 100) & \n",
    "#                                                (Kl_follwrs['followers_count'] > 50) &\n",
    "#                                                (Kl_follwrs['location'].str.contains(r\"Kiev|Kyiv|Київ|Киев\"))].sort_values(by='followers_count',\n",
    "#                                                                                                     ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ww= get_account_tweets('AnastasijaKaram', max_num_twts=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cit_obl = ['Біла Церква', 'Бровари', 'Бориспіль', 'Фастів', 'Ірпінь',\n",
    "        'Васильків', 'Боярка', 'Вишневе', 'Обухів',\n",
    "        'Переяслав-Хмельницький', 'Буча', 'Славутич', 'Яготин', 'Вишгород',\n",
    "        'Сквира', 'Березань', 'Богуслав', 'Тетіїв', 'Українка', 'Кагарлик',\n",
    "        'Тараща', 'Миронівка', 'Узин', 'Ржищів', 'Чорнобиль', \"Прип'ять\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:06\n"
     ]
    }
   ],
   "source": [
    "acc_ntw = get_account_network('huyova_bc', rel_type='followers', max_num =200, key_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_data = [(acc._json['followers_count'], \n",
    "  acc._json['statuses_count'], \n",
    "  acc._json['screen_name'], \n",
    "  acc._json['location'], \n",
    "  acc._json['lang']) for acc in acc_ntw]\n",
    "\n",
    "df = pd.DataFrame(list_data, columns = ['followers_count','statuses_count','screen_name', 'location', 'lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by='followers_count', ascending=False)\n",
    "df_sorted['location'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPEND DATA TO NODE IN HDF FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_try = pd.DataFrame({'a':range(10,20), 'b': range(30,40)}, \n",
    "                      index = random.sample(range(100), 10))\n",
    "df_try2 = pd.DataFrame({'a':[555,777], 'b': [888, 9999]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_try.to_hdf('append_try.h5', '/node_1/b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = df_try.append(df_try2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df.to_hdf('append_try.h5', '/node_1/b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.read_hdf('append_try.h5', '/node_1/b1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET NODES IN HDF FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('lang_data.h5') as f:\n",
    "    list_nodes = f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPROVING LANGUAGE DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to filter out URLs and accounts, not relevant for language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ukraine is a complex country  say    I agree'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"@dijdoer Ukraine is a complex country http://ewdowide.ewd.ewde say @jeiwo @ pgvila http://ewdowide.ewd.ewde I agree\"\n",
    "re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = pd.read_hdf('lang_data.h5', '/ukr_nodes/news/BBC_ua/tls_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "follws_BBC = pd.read_hdf('lang_data.h5', '/ukr_nodes/news/BBC_ua/followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#follws_BBC['location'][follws_BBC['location'].str.contains(r\"Україна|Ukraine|Украина|Київ|Киев\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_data = follws_BBC[follws_BBC['location'].str.contains(r\"Україна|Ukraine|Украина|Київ|Киев\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_txts = [re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", txt)\n",
    "              for txt in aa['texts'].values[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tup_lang_txt = [(txt,detect(txt)) for txt in clean_txts if len(txt) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru_txts = [x[0] for x in tup_lang_txt if x[1] == 'ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ukr_txts = [x[0] for x in tup_lang_txt if x[1] == 'uk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 205)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru_txts), len(ukr_txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_ru =[Counter([detect(x) for _ in range(20)]) for x in ru_txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_uk =[Counter([detect(x) for _ in range(20)]) for x in ukr_txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ru_uncertain = [x for x,y in zip(ru_txts, stats_ru) if y['ru'] < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ukr_uncertain = [x for x,y in zip(ukr_txts, stats_uk) if y['uk'] < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(x, y['ru']) for x,y in zip(ru_txts, stats_ru) if y['ru'] < 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(x, y['uk']) for x,y in zip(ukr_txts, stats_uk) if y['uk'] < 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_data = pd.DataFrame({'a':np.random.randint(1,100,10), 'b':np.random.randint(1,100,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store2 = pd.HDFStore('dummy_ex.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/node_c/subfolder (Group) ''\n",
       "  children := ['block0_values' (Array), 'axis1' (Array), 'block0_items' (Array), 'axis0' (Array)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store2.get_node('/node_c/subfolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store2['node_a'] = dummy_data['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store2['node_b'] = dummy_data['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/node_a', '/node_b', '/node_c/subfolder']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore('dummy_ex.h5', ) as g:\n",
    "    #g.get_node('node_c/subfolder')._f_rename('node_c/subfolder1')\n",
    "    print(g.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame({'c':np.random.randint(1,100,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_.to_hdf('dummy_ex.h5', 'node_c/subfolder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random tweets from a given coordinate box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "data_list, texts, langs, locs = [], [], [], []\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets are the received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_list = []\n",
    "        self.texts = []\n",
    "        self.langs = []\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        jd = json.loads(data)\n",
    "        self.data_list.append(jd)\n",
    "        self.texts.append(jd['text'])\n",
    "        self.langs.append(jd['lang'])\n",
    "        try:\n",
    "            print(data)\n",
    "            saveFile = open('newtweets.csv', 'a')\n",
    "            saveFile.write(data).encode(\"utf8\")\n",
    "            saveFile.write('/n').encode(\"utf8\")\n",
    "            saveFile.close()\n",
    "            return True\n",
    "        except BaseException:\n",
    "            print ('failed ondata')\n",
    "            time.sleep(5)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coordinates\n",
    "Lviv = [23.882904,49.763526,24.163055,49.921167]\n",
    "Kiev = [30.449982,50.408518,30.639496,50.495958]\n",
    "Yerevan = [44.329834,40.078071,44.681396,40.296287]\n",
    "Brussel = [4.258575,50.788575,4.489288,50.913424]\n",
    "Barcelona = [1.835403,41.375778,2.241898,41.586688]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Barcelona\n",
    "# l_Barc = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Barc = Stream(auth, l_Barc)\n",
    "# stream_Barc.filter(locations=Barcelona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counter(l_Barc.langs)\n",
    "# for data, lang in zip(l_Barc.data_list, l_Barc.langs):\n",
    "#     print(data['user']['location'], lang)\n",
    "for text in l_Barc.texts:\n",
    "    if type(text) == str:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for ee in l_Barc.data_list:\n",
    "#     print(ee['place']['id'], ee['place']['place_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Brussel\n",
    "# l_Bru = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Bru = Stream(auth, l_Bru)\n",
    "# stream_Bru.filter(locations=Brussel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Counter(l_Bru.langs)\n",
    "# for data, lang in zip(l_Bru.data_list, l_Bru.langs):\n",
    "#     print(data['user']['location'], lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #LVIV\n",
    "# l_Lv = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Lv = Stream(auth, l_Lv)\n",
    "# stream_Lv.filter(locations=Lviv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #YEREVAN\n",
    "# l_Yer = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Yer = Stream(auth, l_Yer)\n",
    "# stream_Yer.filter(locations=Yerevan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for data, lang in zip(l_Yer.data_list, l_Yer.langs):\n",
    "#     print(data['user']['location'], lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for text in l_Yer.texts:\n",
    "#     if type(text) == str:\n",
    "#         print(text)\n",
    "#     else:\n",
    "#         print(text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for data, lang in zip(data_list, langs):\n",
    "#     print(data['user']['location'], lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KIEV\n",
    "l_Kiev = StdOutListener()\n",
    "#ASK FOR KEYWORD TO COLLECT DATA\n",
    "stream_Kiev = Stream(auth, l_Kiev)\n",
    "stream_Kiev.filter(locations=Kiev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counter(l_Kiev.langs)\n",
    "# for data, lang in zip(l_Kiev.data_list, l_Kiev.langs):\n",
    "#     print(data['user']['location'], lang)\n",
    "for text in l_Kiev.texts:\n",
    "    if type(text) == str:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data in l_Kiev.data_list:\n",
    "    print(data['place']['id'], data['place']['place_type'], \n",
    "          data['place']['country'], data['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    if type(text) == str:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(text.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DYNAMO KIEV PLAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DYNAMO PLAYERS\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"http://www.worldfootball.net/teams/dinamo-kiev/2017/2/\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, \"lxml\")\n",
    "\n",
    "# dynamo_players = pd.read_html(url, encoding='utf8')[1][2].dropna().value\n",
    "dynamo_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dyn_play_countr = pd.read_html(url, encoding='utf8')[1][[2,4]].dropna().values\n",
    "\n",
    "dyn_play_countr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYMONGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "conn=MongoClient()\n",
    "\n",
    "#define database\n",
    "db = conn.citylangs\n",
    "\n",
    "#define collection inside database\n",
    "collection = db.bcn.ajuntam.followers\n",
    "\n",
    "# function to add documents to collection \n",
    "def make_followers_collection(account_name, collection, max_num=100):\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    users = tweepy.Cursor(api.followers, screen_name=account_name).items(max_num)\n",
    "    #i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            user = next(users)\n",
    "            collection.insert_one(user._json)\n",
    "        except tweepy.TweepError as e:\n",
    "            if 'Read timed out' in str(e):\n",
    "                print('fall here')\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                time.sleep(60*16)\n",
    "                user = next(users)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        #print (\"@\" + user.screen_name)\n",
    "    #return collection\n",
    "\n",
    "make_followers_collection('bcn_ajuntament',collection, max_num=100)\n",
    "\n",
    "#check what databases are available\n",
    "conn.database_names()\n",
    "\n",
    "# available collections inside db\n",
    "db.collection_names()\n",
    "\n",
    "collection2 = db.kiev.ukrpravda\n",
    "\n",
    "make_followers_collection('ukrpravda_news',collection2, max_num=300)\n",
    "\n",
    "l = list(conn.citylangs.kiev.ukrpravda.find())\n",
    "\n",
    "Counter([obj['lang'] for obj in l])\n",
    "\n",
    "\n",
    "db.collection_names()\n",
    "\n",
    "#db.categories.insert_one({ \"_id\": \"ukr_pravda\", \"children\": [] })\n",
    "# db.categories.insert({ _id: \"avto_kiev\", children: [] })\n",
    "# db.categories.insert({ _id: \"kiev\", children: [\"ukr_pravda\", \"avto_kiev\"] })\n",
    "\n",
    "db['kiev'].insert_one({'avto_kiev':[],'vitklitschko':[]})\n",
    "\n",
    "db.collection_names()\n",
    "\n",
    "rr=list(db['bcn.ajuntam.followers'].find())\n",
    "\n",
    "db['kiev'].find_one()\n",
    "\n",
    "db['bcn.ajuntam.followers']\n",
    "\n",
    "conn.database_names()\n",
    "\n",
    "coll2 = db.countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW TO LOAD AND SAVE DATA FROM/TO HDF USING PANDAS : STORE, READ_HDF, TO_HDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store3 = pd.HDFStore('dummy_example.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store3['a'] = pd.DataFrame([1,2,3])\n",
    "store3['b'] = pd.DataFrame([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  2\n",
       "2  3"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf('dummy_example.h5', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('dummy_example.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/b', '/c', '/dd/ee', '/dd/ee/123']"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0  15\n",
       "1  11\n",
       "2  10\n",
       "3  15\n",
       "4  14"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['/dd/ee/123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/tables/path.py:100: NaturalNameWarning: object name is not a valid Python identifier: '123'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  NaturalNameWarning)\n"
     ]
    }
   ],
   "source": [
    "x=pd.DataFrame(np.random.randint(10,20,5))\n",
    "x.to_hdf('dummy_example.h5', 'dd/ee/123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string1 = \"ewfw[uim]wqw[erty]weabbatraaaa[wdqwdaddacsdvsdc]qwdeoddowrfre\"\n",
    "string2 = \"dedwwww[qdew]cewc\"\n",
    "string3 = \"[dew]ciao[rferf]sonopaolo[rferfe]come[dwe]va\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uim', 'erty', 'wdqwdaddacsdvsdc']"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\[(\\w+)\\]\", string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx=re.findall(r\"[^(\\[\\w+\\])]\", string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_out_brack():\n",
    "    return re.split(r\"\\[|\\]\", string3)[::2]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'ciao', 'sonopaolo', 'come', 'va']"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_out_brack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ciao', 'sonopaolo', 'come', 'va']"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\w+\",' '.join([y for x in re.findall(r\"(\\w*)\\[\\w+\\](\\w*)\", string3) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ewfw', 'wqw', 'weabbatraaaa', 'qwdeoddow']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y for x in xx for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r\"*\\w*(\\w)(\\w)\\3\\2\\w*\\[(\\w+)\\]\\w*(\\w)(\\w)\\5\\4\\w*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dedwwww', 'qdew', 'cewc']"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"\\[|\\]\", string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-485-d42193c230ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(\\w)(\\w)\\2\\1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "[re.findall(r\"(\\w)(\\w)\\2\\1\", x) for x in grs if len(x) == 4][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# api = tweepy.API(auth)\n",
    "\n",
    "# public_tweets = api.home_timeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txts = [re.sub(r\"(@\\s?[^\\s]+|https?://?[^\\s]+)\", \"\", twt.text)  for twt in public_tweets]\n",
    "\n",
    "Counter([detect(txt) for txt in txts if re.sub(\" \",\"\", txt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_user = api.get_user('budgetvoyage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_user_friends = [friend for friend in dummy_user.friends(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_user_followers = dummy_user.followers(count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OLytovka', 'Киев'), ('ViaSotnyk', 'Украина, Киев')]"
      ]
     },
     "execution_count": 1145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(acc.screen_name, acc.location) for acc in dummy_user_followers if re.findall(r\"Киев|Kiev|Kyiv|Київ\", acc.location) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Оборудование для супермаркетов, магазинов шаговой доступности и ресторанов'"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_user_followers[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_flws = api.followers(count=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[flw.screen_name for flw in my_flws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "web_page = requests.get(\"http://zik.ua/news/2015/05/18/vynnychuk_kyiv_v_okupatsii_rosiyskogo_ukrainska_mova_u_stolytsi__na_zadvirkah_590723\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(web_page.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['&', '%', '#', ':', '!', '.', '-']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"& % #ewoihdoew wefjerpfjr l'Arnau va dir: de cap manera ! ? . Ves-hi tu\"\n",
    "re.findall(r\"[^\\w\\s'?]+\", mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rpeofjer.ferfkepr//fkpe'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rpeofjer. ferfkepr// fkpe\".replace(r\" \", \"\").replace(r\"[^\\w\\s'’,.!?]+\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rpeofjer. ferfkepr fkpe?'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[^\\w\\s'’,.!?]+\", \"\", \"#rpeofjer. ferfke:pr// fkpe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
