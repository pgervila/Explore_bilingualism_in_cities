{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Stream, OAuthHandler, StreamListener\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import pyprind\n",
    "import deepdish as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import secret codes\n",
    "from twitter_pwd import access_token, access_token_secret, consumer_key, consumer_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get random tweets from a given coordinate box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "data_list, texts, langs, locs = [], [], [], []\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets are the received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_list = []\n",
    "        self.texts = []\n",
    "        self.langs = []\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        jd = json.loads(data)\n",
    "        self.data_list.append(jd)\n",
    "        self.texts.append(jd['text'])\n",
    "        self.langs.append(jd['lang'])\n",
    "        try:\n",
    "            print(data)\n",
    "            saveFile = open('newtweets.csv', 'a')\n",
    "            saveFile.write(data).encode(\"utf8\")\n",
    "            saveFile.write('/n').encode(\"utf8\")\n",
    "            saveFile.close()\n",
    "            return True\n",
    "        except BaseException:\n",
    "            print ('failed ondata')\n",
    "            time.sleep(5)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coordinates\n",
    "Lviv = [23.882904,49.763526,24.163055,49.921167]\n",
    "Kiev = [30.449982,50.408518,30.639496,50.495958]\n",
    "Yerevan = [44.329834,40.078071,44.681396,40.296287]\n",
    "Brussel = [4.258575,50.788575,4.489288,50.913424]\n",
    "Barcelona = [1.835403,41.375778,2.241898,41.586688]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Barcelona\n",
    "# l_Barc = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Barc = Stream(auth, l_Barc)\n",
    "# stream_Barc.filter(locations=Barcelona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Counter(l_Barc.langs)\n",
    "# for data, lang in zip(l_Barc.data_list, l_Barc.langs):\n",
    "#     print(data['user']['location'], lang)\n",
    "for text in l_Barc.texts:\n",
    "    if type(text) == str:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for ee in l_Barc.data_list:\n",
    "#     print(ee['place']['id'], ee['place']['place_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Brussel\n",
    "# l_Bru = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Bru = Stream(auth, l_Bru)\n",
    "# stream_Bru.filter(locations=Brussel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Counter(l_Bru.langs)\n",
    "# for data, lang in zip(l_Bru.data_list, l_Bru.langs):\n",
    "#     print(data['user']['location'], lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #LVIV\n",
    "# l_Lv = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Lv = Stream(auth, l_Lv)\n",
    "# stream_Lv.filter(locations=Lviv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #YEREVAN\n",
    "# l_Yer = StdOutListener()\n",
    "# #ASK FOR KEYWORD TO COLLECT DATA\n",
    "# stream_Yer = Stream(auth, l_Yer)\n",
    "# stream_Yer.filter(locations=Yerevan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for data, lang in zip(l_Yer.data_list, l_Yer.langs):\n",
    "#     print(data['user']['location'], lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for text in l_Yer.texts:\n",
    "#     if type(text) == str:\n",
    "#         print(text)\n",
    "#     else:\n",
    "#         print(text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for data, lang in zip(data_list, langs):\n",
    "#     print(data['user']['location'], lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KIEV\n",
    "l_Kiev = StdOutListener()\n",
    "#ASK FOR KEYWORD TO COLLECT DATA\n",
    "stream_Kiev = Stream(auth, l_Kiev)\n",
    "stream_Kiev.filter(locations=Kiev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Counter(l_Kiev.langs)\n",
    "# for data, lang in zip(l_Kiev.data_list, l_Kiev.langs):\n",
    "#     print(data['user']['location'], lang)\n",
    "for text in l_Kiev.texts:\n",
    "    if type(text) == str:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data in l_Kiev.data_list:\n",
    "    print(data['place']['id'], data['place']['place_type'], \n",
    "          data['place']['country'], data['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    if type(text) == str:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(text.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEY FUNCTIONS : USERS, FOLLOWERS, TIMELINES, LANGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_account_network(account_name, rel_type='followers', max_num =100, key_words=None):\n",
    "    pbar = pyprind.ProgBar(max_num)\n",
    "    list_people = []\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    users = tweepy.Cursor(getattr(api, rel_type, 0), screen_name=account_name).items(max_num)\n",
    "    while True:\n",
    "        try:\n",
    "            user = next(users)\n",
    "            if not key_words:\n",
    "                list_people.append(user)\n",
    "            else:\n",
    "                locs = '|'.join(key_words)\n",
    "                patt = re.compile(locs)\n",
    "                found_loc = re.findall(patt, user._json['location'])\n",
    "                if found_loc:\n",
    "                    list_people.append(user)\n",
    "        except tweepy.TweepError as e:\n",
    "            if 'Read timed out' in str(e):\n",
    "                print('fallen here')\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                time.sleep(60*16)\n",
    "                user = next(users)\n",
    "        except StopIteration:\n",
    "            break            \n",
    "        pbar.update()\n",
    "    return list_people\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_account_timeline(account_name, max_num_twts=10):\n",
    "    \"\"\" Given an account name,\n",
    "        it retrieves a maximum number of tweets stored in a list\n",
    "        Args:\n",
    "            * account name: string. Screen_name that identifies the twitter account\n",
    "            * max_num_twts: integer. Maximum number of tweets to be retrieved for each account\n",
    "        Returns:\n",
    "            * list_tweets: list of strings including all retrieved tweets\"\"\"\n",
    "    list_tweets=[]\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    timeline = tweepy.Cursor(api.user_timeline, screen_name=account_name).items(max_num_twts)\n",
    "    i=0\n",
    "    while True:\n",
    "        try:\n",
    "            tw = next(timeline)\n",
    "            list_tweets.append(tw)\n",
    "        except tweepy.TweepError as e:\n",
    "            if '401' in str(e):    \n",
    "                print(e)\n",
    "                time.sleep(10)\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(60*15)\n",
    "                tw = next(timeline)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return list_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twts_from_list_account_names(list_accounts, max_num_accounts=None, max_num_twts=10):\n",
    "    pbar = pyprind.ProgBar(len(list_accounts))\n",
    "    texts_tweets = []\n",
    "    langs_tweets = []\n",
    "    authors_tweets = []\n",
    "    if max_num_accounts:\n",
    "        list_accounts = list_accounts[:max_num_accounts]\n",
    "    for idx, f in enumerate(list_accounts):\n",
    "        #print(idx)\n",
    "        tl = get_account_timeline(f, max_num_twts=max_num_twts)\n",
    "        texts_tweets.extend([tw.text for tw in tl])\n",
    "        langs_tweets.extend([tw.lang for tw in tl])\n",
    "        authors_tweets.extend([f for _ in tl])\n",
    "        pbar.update()\n",
    "    return texts_tweets, langs_tweets, authors_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UKRAINE: data structure and relevant twitter accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ukraine_nodes = {}\n",
    "Ukraine_nodes['cities'] = ['kiev', 'odessa', 'lviv', 'kharkov', 'dnipropetrovsk']\n",
    "Ukraine_nodes['city_sites'] = {'Mariupol':['0629ComUa'], \n",
    "                               'kiev':['kievtypical','kliniki_kiev','LISOD_clinic','avto_kiev', 'editbeauty']}\n",
    "Ukraine_nodes['news'] = ['HromadskeUA','tsnua','ukrpravda_news', 'lb_ua', 'Korrespondent', \n",
    "                         'Delo_ua', 'BBC_ua', 'LIGAnet', 'segodnya_life']\n",
    "Ukraine_nodes['TV'] = ['5channel', 'EspresoTV', '24tvua', 'footballua_tv']\n",
    "Ukraine_nodes['starsystem'] = ['VeraBrezhneva', 's_vakarchuk', 'KAMEHCKUX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_words=['–£–∫—Ä–∞—ó–Ω–∞', 'Ukraine', '–£–∫—Ä–∞–∏–Ω–∞', '–ö–∏—ó–≤', '–ö–∏–µ–≤']\n",
    "HromadskeUA_followers = get_account_network('HromadskeUA', rel_type='followers', \n",
    "                                            max_num =5000, key_words=key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[#                             ] | ETA: 00:01:56"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 895\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###                           ] | ETA: 02:20:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 896\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#####                         ] | ETA: 02:35:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 894\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#######                       ] | ETA: 02:35:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 894\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#########                     ] | ETA: 02:26:43"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 894\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##########                    ] | ETA: 02:36:35"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 894\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############                  ] | ETA: 02:20:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 895\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############                ] | ETA: 02:04:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 891\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[################              ] | ETA: 01:49:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 892\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################            ] | ETA: 01:34:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 892\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###################           ] | ETA: 01:31:16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 893\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#####################         ] | ETA: 01:14:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 884\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#######################       ] | ETA: 00:57:36"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 892\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#########################     ] | ETA: 00:41:04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 893\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[###########################   ] | ETA: 00:24:32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 894\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############################  ] | ETA: 00:16:52"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Rate limit reached. Sleeping for: 892\n",
      "fallen here"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "\n",
      "Total time elapsed: 04:12:35\n",
      "//anaconda/lib/python3.5/site-packages/pandas/core/generic.py:939: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->['created_at', 'description', 'entities', 'id_str', 'lang', 'location', 'name', 'profile_background_color', 'profile_background_image_url', 'profile_background_image_url_https', 'profile_banner_url', 'profile_image_url', 'profile_image_url_https', 'profile_link_color', 'profile_sidebar_border_color', 'profile_sidebar_fill_color', 'profile_text_color', 'screen_name', 'status', 'time_zone', 'translator_type', 'url']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n"
     ]
    }
   ],
   "source": [
    "country = 'ukr'\n",
    "node_name = 'starsystem'\n",
    "acc_name = 's_vakarchuk'\n",
    "rel_type = 'followers'\n",
    "\n",
    "#key_words=['–£–∫—Ä–∞—ó–Ω–∞', 'Ukraine', '–£–∫—Ä–∞–∏–Ω–∞', '–ö–∏—ó–≤', '–ö–∏–µ–≤']\n",
    "path_save = '/'.join(['',country, node_name, acc_name, rel_type])\n",
    "followers = get_account_network(acc_name, rel_type=rel_type, \n",
    "                                max_num =5000, key_words=None)\n",
    "json_format = [elem._json for elem in followers]\n",
    "df = pd.DataFrame(json_format)\n",
    "df.to_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_name = 'LIGAnet'\n",
    "df = pd.read_hdf('lang_data.h5', '/ukr_nodes/news/' + acc_name + '/followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ru    420\n",
       "uk    154\n",
       "en     21\n",
       "it      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lang'][df['location'].str.contains(r\"(–£–∫—Ä–∞—ó–Ω–∞|Ukraine|–£–∫—Ä–∞–∏–Ω–∞|–ö–∏—ó–≤|–ö–∏–µ–≤)\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "screen_name        501\n",
       "statuses_count     501\n",
       "followers_count    501\n",
       "dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[df['statuses_count'] >= 5][['screen_name', 'statuses_count','followers_count']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HRMUA_flwrs = [f for idx, f in df_HRMUA.iterrows()]\n",
    "#HRMUA_texts, HRMUA_langs = get_twts_from_list_account_names(HRMUA_flwrs, max_num_followers=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_txts_langs_HRMUA = pd.DataFrame({'texts':HRMUA_texts, 'lang':HRMUA_langs})\n",
    "\n",
    "df_txts_langs_HRMUA.to_hdf('lang_data.h5', '/ukr_nodes/news/HromadskeUA/tls_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_txts = pd.read_hdf('lang_data.h5', '/ukr_nodes/news/HromadskeUA/tls_followers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATALONIA NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Catalonia_nodes = {}\n",
    "Catalonia_nodes['news'] = ['LaVanguardia', 'VilaWeb', 'diariARA', 'elperiodico',\n",
    "                           'elperiodico_cat', 'elpuntavui']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country = 'cat'\n",
    "node_name = 'news'\n",
    "acc_name = 'elperiodico'\n",
    "rel_type = 'followers'\n",
    "\n",
    "path_save = '/'.join(['',country, node_name, acc_name, rel_type])\n",
    "followers = get_account_network(acc_name, rel_type=rel_type, \n",
    "                                max_num =5000, key_words=None)\n",
    "json_format = [elem._json for elem in followers]\n",
    "df = pd.DataFrame(json_format)\n",
    "df.to_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_words=['Catal', 'Bcn', 'Barcel']\n",
    "followers = get_account_network(screen_name, rel_type='followers', \n",
    "                                max_num =5000, key_words=None)\n",
    "json_info = [elem._json for elem in followers]\n",
    "df = pd.DataFrame(json_info)\n",
    "df.to_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ARA_followers = get_account_network('diariARA', rel_type='followers', \n",
    "                                       max_num =5000, key_words=None)\n",
    "json_info_ARA = [elem._json for elem in ARA_followers]\n",
    "df_ARA = pd.DataFrame(json_info_ARA)\n",
    "df_ARA.to_hdf('lang_data.h5', '/cat_nodes/news/diariARA/followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ARA_follwrs = pd.read_hdf('lang_data.h5', '/cat_nodes/news/diariARA/followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "es       240\n",
       "ca       178\n",
       "en        41\n",
       "it         1\n",
       "fr         1\n",
       "en-GB      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ARA_follwrs[df_ARA_follwrs['statuses_count'] > 5][['screen_name', 'statuses_count','followers_count']].count()\n",
    "df_ARA_follwrs['lang'][df_ARA_follwrs['statuses_count'] > 5][df_ARA_follwrs['location'].str.contains(r\"(Barcel|Catal)\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "counts_ARA_Cat = df_ARA['lang'][df_ARA['location'].str.contains(r\"(Barcel|Catal)\")].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13139999999999999"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ARA['lang'][df_ARA['location'].str.contains(r\"(Barcel|Catal|Tarr|Llei|Ger|Gir|Badal)\")].count()/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_LaVang[['lang', 'screen_name']][df_LaVang['location'].str.contains(r\"(Barcel|Catal)\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET TWEETS FROM FOLLOWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:15: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79,)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'ukr'\n",
    "node_name = 'starsystem'\n",
    "screen_name = 's_vakarchuk'\n",
    "rel_type1 = 'followers'\n",
    "rel_type2 = 'tls_followers'\n",
    "min_num_twts_per_acc=5\n",
    "\n",
    "path_load = '/'.join(['',country, node_name, screen_name, rel_type1])\n",
    "path_save = '/'.join(['',country, node_name, screen_name, rel_type2])\n",
    "\n",
    "key_words = {'ukr':r\"(–£–∫—Ä–∞—ó–Ω–∞|Ukraine|–£–∫—Ä–∞–∏–Ω–∞|–ö–∏—ó–≤|–ö–∏–µ–≤|–õ—å–≤—ñ–≤|–û–¥–µ—Å)\", \n",
    "             'cat':r\"(Barcel|Catal|Tarr|Llei|Ger|Gir|Badal)\"}\n",
    "df = pd.read_hdf('lang_data.h5', path_load)\n",
    "relevant_followers = df['screen_name'][df['statuses_count'] >= min_num_twts][\n",
    "                         df['location'].str.contains(key_words[country])\n",
    "                       ].values\n",
    "relevant_followers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ukr/starsystem/s_vakarchuk/followers'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tl_followers(screen_name, country, node_name, min_num_twts_per_acc=5, max_num_followers=None):\n",
    "    \"\"\" Creates pandas dataframe with all tweet texts and corresponding language\n",
    "    from followers of a given account. A dataframe with all followers info \n",
    "    must have been previously computed and saved in hdf5 format\"\"\"\n",
    "    base_path = '/'.join(['',country, node_name, screen_name])\n",
    "    path_load = base_path + '/followers'\n",
    "    path_save = base_path + '/tls_followers'\n",
    "    key_words = {'ukr':r\"(–£–∫—Ä–∞—ó–Ω–∞|Ukraine|–£–∫—Ä–∞–∏–Ω–∞|–ö–∏—ó–≤|–ö–∏–µ–≤|–õ—å–≤—ñ–≤|–û–¥–µ—Å)\", \n",
    "                 'cat':r\"(Barcel|Catal|Tarr|Llei|Ger|Gir|Badal)\"}\n",
    "    df = pd.read_hdf('lang_data.h5', path_load)\n",
    "    # filter by num_min_twts_per_account\n",
    "    relevant_followers = df['screen_name'][df['statuses_count'] >= min_num_twts_per_acc]\n",
    "    # keep only country residents\n",
    "    relevant_followers = relevant_followers[df['location'].str.contains(key_words[country])].values\n",
    "    texts, langs, auth = get_twts_from_list_account_names(relevant_followers, \n",
    "                                                          max_num_accounts=max_num_followers)\n",
    "    df_txts_langs= pd.DataFrame({'texts':texts, 'lang':langs, 'screen_name':auth})\n",
    "    df_txts_langs.to_hdf('lang_data.h5', path_save)\n",
    "    return df_txts_langs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = '/'.join(['',country, node_name, screen_name])\n",
    "path_load = base_path + '/followers'\n",
    "path_save = base_path + '/tls_followers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>Hi, my name is Tina‚úã. I'm 16 years old and i'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>It's my brother's birthday today!!! Yeeeeeey)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>When you have nothing to do https://t.co/vdK2e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uk</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>@SolomiaNB –ó–≤—É—á–∏—Ç—å –ø–æ-—Ñ—ñ–ª–æ—Å–æ—Ñ—Å—å–∫–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ru</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>@IloveLHMelovin –ü–æ–¥–∏–≤–∏—Å—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uk</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>@SolomiaNB –ê —Ü–µ –≤–∂–µ –Ω–∞–≥–ª–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>RT @KatieStrope96: Someone please take advanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uk</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>@SolomiaNB –í—ñ–ª—å–Ω–∞(–º—Ä—ñ–π)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ru</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>@SolomiaNB –û—Ç +</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ru</td>\n",
       "      <td>ChristinaVaAn</td>\n",
       "      <td>@Brizer_06 –ï–µ–µ–µ–µ–π, –≤—Å–µ –¥–æ–±—Ä–µ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>–§.–ö–∏—Ä–∫–æ—Ä–æ–≤ –∏ –µ–≥–æ \"–∂–µ–Ω–∞\" https://t.co/WyKyrk6BC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>–§–∞–Ω–∞—Ç–∫–∞ –ö–∏—Ä–∫–æ—Ä–æ–≤–∞ \"—Ä–æ–¥–∏–ª–∞\" –µ–º—É —á–µ—Ç–≤–µ—Ä—ã—Ö –¥–µ—Ç–µ–π ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>–ù–µ –Ω–µ–≤–µ—Å—Ç–∞ –∞ –ø–æ–∑–æ—Ä–∏—â–µ –∫–∞–∫–æ–µ —Ç–æ !!  –î–∞–≤–∞–π –ø–æ–∂–µ–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>@Club_FKirkorova –§–∏–ª–∏–ø–ø —è —Ç–≤–æ—è –∂–µ–Ω–∞üèçÔ∏èüê±ü¶íüöóüö° http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>–í—Å–µ —Ö–æ—Ä–æ—à–æ https://t.co/pE4tPjGNZc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>@Club_FKirkorova –¢–≤–æ—è –∂–µ–Ω–∞ –ö–∞—Ä–æ–ª–∏–Ω–∞ –ö–∏—Ä–∫–æ—Ä–æ–≤–∞ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>–í—Å–µ –æ–∫–µ–π!üê∂üêªüê±ü¶í https://t.co/vCLUjI4Ye5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>@vkontakte –ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º –æ—Ç –ö–∞—Ä–æ–ª–∏–Ω—ã –ö–∏—Ä–∫–æ—Ä–æ–≤–æ–π!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>und</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>https://t.co/irhG18R41n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ru</td>\n",
       "      <td>y16BFIHiXerxBYD</td>\n",
       "      <td>@Pushistik_532 –ü—Ä–∏–≤–µ—Ç –¥–µ–≤—á–æ–Ω—É—à–∫–∞! https://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>RT @gor_lev: 14 –∏—é–Ω—è\\n–õ—å–≤—ã - –µ—â–µ —á—É—Ç—å-—á—É—Ç—å –ø–æ—Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>@CHURIKOVA_ –ü–•–ê–•–ê–ê–•–ê–•–ê–•, –Ω–∞–≤–µ—Ä–Ω–æ–µ –±–æ–ª—å—à–µ —á–µ–º —èüòÇüòÇüòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>–ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è ,—á—Ç–æ –ø–µ—Ä–µ–¥ –ó–ù–û –Ω–µ —Ç–∞–∫ —Å—Ç—Ä—ë–º–Ω–æ –±—ã–ª–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>–†–µ–±—è—Ç–∞,–º–æ–∂–Ω–æ –∫–æ–º—É-—Ç–æ –¥–∞–º –ø–∞—Ä–æ–ª—å –∏ –ª–æ–≥–∏–Ω –æ—Ç –ª–∏—á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>RT @StunningPosts: –∏ –≤–æ—Ç —Ç—ã –≤ –æ—á–µ—Ä–µ–¥–Ω–æ–π —Ä–∞–∑ –¥—É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>RT @VityanOneLove: –ó–∞—á–µ–º –∏–¥—Ç–∏ –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç,–µ—Å–ª–∏ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>@alina_dymar —á–µ–≥–æ —è –Ω–µ –º–æ–≥—É —Ä–µ—Ç–≤–∏—Ç–Ω—É—Ç—å?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>@alina_dymar –í –∏—é–ª–µ‚ù§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ru</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>RT @vacuumxfck: –ñ–∞–ª—å, —á—Ç–æ –≤ –í–∫–æ–Ω—Ç–∞–∫—Ç–µ –Ω–µ—Ç —Ñ—É–Ω–∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>und</td>\n",
       "      <td>nata_voytovich</td>\n",
       "      <td>RT @bvckl: https://t.co/2U0ZgIY7Yb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>@verkhovna_rada –†–∞–¥–∞ –ø—Ä–æ–≥–æ–ª–æ—Å—É–≤–∞–ª–∞ –∑–∞ –º–æ–≤–Ω—ñ –∫–≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>RT @kievtypical: –°—å–æ–≥–æ–¥–Ω—ñ –î–µ–Ω—å –ø–∞–º‚Äô—è—Ç—ñ –∂–µ—Ä—Ç–≤ –ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>–ù–∞–≤—ñ—Ç—å –Ω—ñ—á–æ–≥–æ –¥–æ–¥–∞—Ç–∏.. https://t.co/DKdE7woHxc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>–†–∞–¥–∂—É –ø—Ä–æ—á–∏—Ç–∞—Ç–∏!\\n https://t.co/f2UdmlnuNq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>RT @brutfoot: –ó–ú–Ü: –ì–æ–ª–∫—ñ–ø–µ—Ä \"–ú–∞–Ω—á–µ—Å—Ç–µ—Ä –°—ñ—Ç—ñ\" –ö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>RT @UPZhyttya: NASA –ø–æ–∫–∞–∑–∞–ª–æ –ø—Ä–∏–≥–æ–ª–æ–º—à–ª–∏–≤—ñ –∑–Ω—ñ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>#–ø–µ—Ä–µ–º–æ–≥–∞ #respect Katie Melua –±—Ä–∏—Ç–∞–Ω—Å—å–∫–æ-–≥—Ä—É–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>–ö- –ö—Ä–∏–∞—Ç–∏–≤–Ω–æ –∞–±–æ —è–∫ –ö–∏—ó–≤—Å—å–∫–∏–π –ù–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —É–Ω—ñ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>‚Äú–ù–∞ –ø–æ–ª—å—Å—å–∫–∏—Ö –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤–∞—Ö –ø—Ä–∞—Ü—é—é—Ç—å –≤—ñ–¥ 10% –¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>uk</td>\n",
       "      <td>Borys_ZV</td>\n",
       "      <td>–í—ñ—á–Ω–æ –ø–µ—Ä—à–∏–π –°–∞—à–∞ –®–æ–≤–∫–æ–≤—Å—å–∫–∏–π #–ª–µ–≥–µ–Ω–¥–∞ #–°–∞–®–æ #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>en</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>Trying out @theTunnelBear so I can browse priv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>und</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>RT @OJessicaNigri: üôå‚ù§ https://t.co/0usPTaeUZj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>ru</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>–í–∫–ª—é—á–∏—Ç–µ ¬´–ù–∏—Ä–≤–∞–Ω—É¬ª –ø–æ–≥—Ä–æ–º—á–µ https://t.co/PHVZj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>und</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>–®–∞—Ä—Ñ –°–ù–£–î –∫—Ä—é—á–∫–æ–º –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö Round Crochet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>en</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>RT @MCR_FANS: NEWS: The Black Parade/Living Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>cs</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>Thank you so much for the love —Å –ø–æ–º–æ—â—å—é @MCRo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>cs</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>The Black Parade Is Dead! (Full) - My Chemical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>ru</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>–ò–ù–û–°–¢–†–ê–ù–¶–´ –°–õ–£–®–ê–Æ–¢ –†–£–°–°–ö–£–Æ –ú–£–ó–´–ö–£ #4 https://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>ru</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>–ö–∞–∫ –ë—ã—Å—Ç—Ä–æ –í—ã—É—á–∏—Ç—å –ê–Ω–≥–ª–∏–π—Å–∫–∏–π https://t.co/tEu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>ru</td>\n",
       "      <td>Lex56679854</td>\n",
       "      <td>–ê–ú–ï–†–ò–ö–ê–ù–¶–´ —Å–º–æ—Ç—Ä—è—Ç OXXXYMIRON - –ì–û–†–û–î –ü–û–î –ü–û–î–û...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ö–æ–≥–æ –∏–∑ –∑–Ω–∞–º–µ–Ω–∏—Ç–æ—Å—Ç–µ–π —Ç—ã –±—ã —Ö–æ—Ç–µ–ª(-–∞) —á–∏—Ç–∞—Ç—å –Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>https://t.co/uSVH0wH463 –≤—Å—Ç—É–ø–∏, –∏ –∞—Ä—Ç –ø–æ–ª—É—á–∏ ‚Äî...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>eu</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>‚ìì‚ìî¬∂‚ò∫ ‚Äî Gofyqysjcjxhshdhdjcj https://t.co/kaIZD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ö—Ç–æ —Ç–≤–æ–π –ª—é–±–∏–º—ã–π —Ö—É–¥–æ–∂–Ω–∏–∫? ‚Äî –ú–æ–π –ª—é–±–∏–º—ã–π\\n–∂—É–¥–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ü—Ä–æ–¥–æ–ª–∂–∏ —Ñ—Ä–∞–∑—É: ‚Äú–ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã, —á—Ç–æ ...‚Äù ‚Äî –í—ã –¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç, –ø–æ–º–æ–≥–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –≥—Ä—É–ø–ø—ã ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç, –ø–æ–º–æ–≥–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –≥—Ä—É–ø–ø—ã ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ó–∞–π–∫–∞, –≤—Å—Ç—É–ø–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ https://t.co/sag700Io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä–æ–π —Ç–µ–±–µ –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å –æ–±–ª–∞–¥–∞—Ç—å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>ru</td>\n",
       "      <td>Dadik_Ma</td>\n",
       "      <td>–ß–∞—Å—Ç–æ –ª–∏ —Ç—ã —Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤–∞–µ—à—å—Å—è –≤ –ª—é–¥—è—Ö? ‚Äî –ï—Å ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>637 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang      screen_name                                              texts\n",
       "0     en    ChristinaVaAn  Hi, my name is Tina‚úã. I'm 16 years old and i'm...\n",
       "1     en    ChristinaVaAn      It's my brother's birthday today!!! Yeeeeeey)\n",
       "2     en    ChristinaVaAn  When you have nothing to do https://t.co/vdK2e...\n",
       "3     uk    ChristinaVaAn                  @SolomiaNB –ó–≤—É—á–∏—Ç—å –ø–æ-—Ñ—ñ–ª–æ—Å–æ—Ñ—Å—å–∫–∏\n",
       "4     ru    ChristinaVaAn                           @IloveLHMelovin –ü–æ–¥–∏–≤–∏—Å—å\n",
       "5     uk    ChristinaVaAn                          @SolomiaNB –ê —Ü–µ –≤–∂–µ –Ω–∞–≥–ª–æ\n",
       "6     en    ChristinaVaAn  RT @KatieStrope96: Someone please take advanta...\n",
       "7     uk    ChristinaVaAn                            @SolomiaNB –í—ñ–ª—å–Ω–∞(–º—Ä—ñ–π)\n",
       "8     ru    ChristinaVaAn                                    @SolomiaNB –û—Ç +\n",
       "9     ru    ChristinaVaAn                      @Brizer_06 –ï–µ–µ–µ–µ–π, –≤—Å–µ –¥–æ–±—Ä–µ?\n",
       "10    ru  y16BFIHiXerxBYD  –§.–ö–∏—Ä–∫–æ—Ä–æ–≤ –∏ –µ–≥–æ \"–∂–µ–Ω–∞\" https://t.co/WyKyrk6BC...\n",
       "11    ru  y16BFIHiXerxBYD  –§–∞–Ω–∞—Ç–∫–∞ –ö–∏—Ä–∫–æ—Ä–æ–≤–∞ \"—Ä–æ–¥–∏–ª–∞\" –µ–º—É —á–µ—Ç–≤–µ—Ä—ã—Ö –¥–µ—Ç–µ–π ...\n",
       "12    ru  y16BFIHiXerxBYD  –ù–µ –Ω–µ–≤–µ—Å—Ç–∞ –∞ –ø–æ–∑–æ—Ä–∏—â–µ –∫–∞–∫–æ–µ —Ç–æ !!  –î–∞–≤–∞–π –ø–æ–∂–µ–Ω...\n",
       "13    ru  y16BFIHiXerxBYD  @Club_FKirkorova –§–∏–ª–∏–ø–ø —è —Ç–≤–æ—è –∂–µ–Ω–∞üèçÔ∏èüê±ü¶íüöóüö° http...\n",
       "14    ru  y16BFIHiXerxBYD                 –í—Å–µ —Ö–æ—Ä–æ—à–æ https://t.co/pE4tPjGNZc\n",
       "15    ru  y16BFIHiXerxBYD  @Club_FKirkorova –¢–≤–æ—è –∂–µ–Ω–∞ –ö–∞—Ä–æ–ª–∏–Ω–∞ –ö–∏—Ä–∫–æ—Ä–æ–≤–∞ ...\n",
       "16    ru  y16BFIHiXerxBYD              –í—Å–µ –æ–∫–µ–π!üê∂üêªüê±ü¶í https://t.co/vCLUjI4Ye5\n",
       "17    ru  y16BFIHiXerxBYD  @vkontakte –ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º –æ—Ç –ö–∞—Ä–æ–ª–∏–Ω—ã –ö–∏—Ä–∫–æ—Ä–æ–≤–æ–π!...\n",
       "18   und  y16BFIHiXerxBYD                            https://t.co/irhG18R41n\n",
       "19    ru  y16BFIHiXerxBYD  @Pushistik_532 –ü—Ä–∏–≤–µ—Ç –¥–µ–≤—á–æ–Ω—É—à–∫–∞! https://t.co...\n",
       "20    ru   nata_voytovich  RT @gor_lev: 14 –∏—é–Ω—è\\n–õ—å–≤—ã - –µ—â–µ —á—É—Ç—å-—á—É—Ç—å –ø–æ—Ç...\n",
       "21    ru   nata_voytovich  @CHURIKOVA_ –ü–•–ê–•–ê–ê–•–ê–•–ê–•, –Ω–∞–≤–µ—Ä–Ω–æ–µ –±–æ–ª—å—à–µ —á–µ–º —èüòÇüòÇüòÇ\n",
       "22    ru   nata_voytovich  –ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è ,—á—Ç–æ –ø–µ—Ä–µ–¥ –ó–ù–û –Ω–µ —Ç–∞–∫ —Å—Ç—Ä—ë–º–Ω–æ –±—ã–ª–æ...\n",
       "23    ru   nata_voytovich  –†–µ–±—è—Ç–∞,–º–æ–∂–Ω–æ –∫–æ–º—É-—Ç–æ –¥–∞–º –ø–∞—Ä–æ–ª—å –∏ –ª–æ–≥–∏–Ω –æ—Ç –ª–∏—á...\n",
       "24    ru   nata_voytovich  RT @StunningPosts: –∏ –≤–æ—Ç —Ç—ã –≤ –æ—á–µ—Ä–µ–¥–Ω–æ–π —Ä–∞–∑ –¥—É...\n",
       "25    ru   nata_voytovich  RT @VityanOneLove: –ó–∞—á–µ–º –∏–¥—Ç–∏ –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç,–µ—Å–ª–∏ ...\n",
       "26    ru   nata_voytovich            @alina_dymar —á–µ–≥–æ —è –Ω–µ –º–æ–≥—É —Ä–µ—Ç–≤–∏—Ç–Ω—É—Ç—å?\n",
       "27    ru   nata_voytovich                               @alina_dymar –í –∏—é–ª–µ‚ù§\n",
       "28    ru   nata_voytovich  RT @vacuumxfck: –ñ–∞–ª—å, —á—Ç–æ –≤ –í–∫–æ–Ω—Ç–∞–∫—Ç–µ –Ω–µ—Ç —Ñ—É–Ω–∫...\n",
       "29   und   nata_voytovich                 RT @bvckl: https://t.co/2U0ZgIY7Yb\n",
       "..   ...              ...                                                ...\n",
       "607   uk         Borys_ZV  @verkhovna_rada –†–∞–¥–∞ –ø—Ä–æ–≥–æ–ª–æ—Å—É–≤–∞–ª–∞ –∑–∞ –º–æ–≤–Ω—ñ –∫–≤...\n",
       "608   uk         Borys_ZV  RT @kievtypical: –°—å–æ–≥–æ–¥–Ω—ñ –î–µ–Ω—å –ø–∞–º‚Äô—è—Ç—ñ –∂–µ—Ä—Ç–≤ –ø...\n",
       "609   uk         Borys_ZV     –ù–∞–≤—ñ—Ç—å –Ω—ñ—á–æ–≥–æ –¥–æ–¥–∞—Ç–∏.. https://t.co/DKdE7woHxc\n",
       "610   uk         Borys_ZV         –†–∞–¥–∂—É –ø—Ä–æ—á–∏—Ç–∞—Ç–∏!\\n https://t.co/f2UdmlnuNq\n",
       "611   uk         Borys_ZV  RT @brutfoot: –ó–ú–Ü: –ì–æ–ª–∫—ñ–ø–µ—Ä \"–ú–∞–Ω—á–µ—Å—Ç–µ—Ä –°—ñ—Ç—ñ\" –ö...\n",
       "612   uk         Borys_ZV  RT @UPZhyttya: NASA –ø–æ–∫–∞–∑–∞–ª–æ –ø—Ä–∏–≥–æ–ª–æ–º—à–ª–∏–≤—ñ –∑–Ω—ñ...\n",
       "613   uk         Borys_ZV  #–ø–µ—Ä–µ–º–æ–≥–∞ #respect Katie Melua –±—Ä–∏—Ç–∞–Ω—Å—å–∫–æ-–≥—Ä—É–∑...\n",
       "614   uk         Borys_ZV  –ö- –ö—Ä–∏–∞—Ç–∏–≤–Ω–æ –∞–±–æ —è–∫ –ö–∏—ó–≤—Å—å–∫–∏–π –ù–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —É–Ω—ñ...\n",
       "615   uk         Borys_ZV  ‚Äú–ù–∞ –ø–æ–ª—å—Å—å–∫–∏—Ö –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤–∞—Ö –ø—Ä–∞—Ü—é—é—Ç—å –≤—ñ–¥ 10% –¥...\n",
       "616   uk         Borys_ZV  –í—ñ—á–Ω–æ –ø–µ—Ä—à–∏–π –°–∞—à–∞ –®–æ–≤–∫–æ–≤—Å—å–∫–∏–π #–ª–µ–≥–µ–Ω–¥–∞ #–°–∞–®–æ #...\n",
       "617   en      Lex56679854  Trying out @theTunnelBear so I can browse priv...\n",
       "618  und      Lex56679854      RT @OJessicaNigri: üôå‚ù§ https://t.co/0usPTaeUZj\n",
       "619   ru      Lex56679854  –í–∫–ª—é—á–∏—Ç–µ ¬´–ù–∏—Ä–≤–∞–Ω—É¬ª –ø–æ–≥—Ä–æ–º—á–µ https://t.co/PHVZj...\n",
       "620  und      Lex56679854  –®–∞—Ä—Ñ –°–ù–£–î –∫—Ä—é—á–∫–æ–º –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö Round Crochet...\n",
       "621   en      Lex56679854  RT @MCR_FANS: NEWS: The Black Parade/Living Wi...\n",
       "622   cs      Lex56679854  Thank you so much for the love —Å –ø–æ–º–æ—â—å—é @MCRo...\n",
       "623   cs      Lex56679854  The Black Parade Is Dead! (Full) - My Chemical...\n",
       "624   ru      Lex56679854  –ò–ù–û–°–¢–†–ê–ù–¶–´ –°–õ–£–®–ê–Æ–¢ –†–£–°–°–ö–£–Æ –ú–£–ó–´–ö–£ #4 https://t...\n",
       "625   ru      Lex56679854  –ö–∞–∫ –ë—ã—Å—Ç—Ä–æ –í—ã—É—á–∏—Ç—å –ê–Ω–≥–ª–∏–π—Å–∫–∏–π https://t.co/tEu...\n",
       "626   ru      Lex56679854  –ê–ú–ï–†–ò–ö–ê–ù–¶–´ —Å–º–æ—Ç—Ä—è—Ç OXXXYMIRON - –ì–û–†–û–î –ü–û–î –ü–û–î–û...\n",
       "627   ru         Dadik_Ma  –ö–æ–≥–æ –∏–∑ –∑–Ω–∞–º–µ–Ω–∏—Ç–æ—Å—Ç–µ–π —Ç—ã –±—ã —Ö–æ—Ç–µ–ª(-–∞) —á–∏—Ç–∞—Ç—å –Ω...\n",
       "628   ru         Dadik_Ma  https://t.co/uSVH0wH463 –≤—Å—Ç—É–ø–∏, –∏ –∞—Ä—Ç –ø–æ–ª—É—á–∏ ‚Äî...\n",
       "629   eu         Dadik_Ma  ‚ìì‚ìî¬∂‚ò∫ ‚Äî Gofyqysjcjxhshdhdjcj https://t.co/kaIZD...\n",
       "630   ru         Dadik_Ma  –ö—Ç–æ —Ç–≤–æ–π –ª—é–±–∏–º—ã–π —Ö—É–¥–æ–∂–Ω–∏–∫? ‚Äî –ú–æ–π –ª—é–±–∏–º—ã–π\\n–∂—É–¥–æ...\n",
       "631   ru         Dadik_Ma  –ü—Ä–æ–¥–æ–ª–∂–∏ —Ñ—Ä–∞–∑—É: ‚Äú–ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã, —á—Ç–æ ...‚Äù ‚Äî –í—ã –¥...\n",
       "632   ru         Dadik_Ma  –ü—Ä–∏–≤–µ—Ç, –ø–æ–º–æ–≥–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –≥—Ä—É–ø–ø—ã ht...\n",
       "633   ru         Dadik_Ma  –ü—Ä–∏–≤–µ—Ç, –ø–æ–º–æ–≥–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –≥—Ä—É–ø–ø—ã ht...\n",
       "634   ru         Dadik_Ma  –ó–∞–π–∫–∞, –≤—Å—Ç—É–ø–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ https://t.co/sag700Io...\n",
       "635   ru         Dadik_Ma  –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä–æ–π —Ç–µ–±–µ –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å –æ–±–ª–∞–¥–∞—Ç—å...\n",
       "636   ru         Dadik_Ma  –ß–∞—Å—Ç–æ –ª–∏ —Ç—ã —Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤–∞–µ—à—å—Å—è –≤ –ª—é–¥—è—Ö? ‚Äî –ï—Å ht...\n",
       "\n",
       "[637 rows x 3 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "# filter followers to focus on most relevant ones\n",
    "min_num_twts = 5\n",
    "relevant_followers = df['screen_name'][df['statuses_count'] >= min_num_twts][\n",
    "                         df['location'].str.contains(key_words[country])\n",
    "                       ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[#####                         ] | ETA: 00:01:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter error response: status code = 401\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#########                     ] | ETA: 00:01:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#############                 ] | ETA: 00:01:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############                ] | ETA: 00:01:27"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[####################          ] | ETA: 00:00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################        ] | ETA: 00:00:42"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################      ] | ETA: 00:00:33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter error response: status code = 401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############################] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:02:40\n"
     ]
    }
   ],
   "source": [
    "# get twts, lang, authors. Transform to pandas df and save\n",
    "texts, langs, auth = get_twts_from_list_account_names(relevant_followers, max_num_followers=79)\n",
    "df_txts_langs= pd.DataFrame({'texts':texts, 'lang':langs, 'screen_name':auth})\n",
    "df_txts_langs.to_hdf('lang_data.h5', path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_txts_langs['lang'].value_counts()\n",
    "#langs_detected = [detect(txt) for txt in df_txts_langs['texts']]\n",
    "\n",
    "langs_detected=[]\n",
    "for txt in df_txts_langs['texts']:\n",
    "    try:\n",
    "        langs_detected.append(detect(txt))\n",
    "    except:\n",
    "        langs_detected.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_try = pd.DataFrame({'a':['aaaa','bbbfdde',1],'b':[23,44,56]})\n",
    "df_try2 = pd.DataFrame({'a':['xxxx','zzzz'],'b':[3233,43214]})\n",
    "\n",
    "store = pd.HDFStore('try_hyerar.h5')\n",
    "\n",
    "store.append('city/topic', df_try)\n",
    "\n",
    "store.close()\n",
    "\n",
    "pd.read_hdf('try_hyerar.h5', 'city/topic')\n",
    "\n",
    "store = pd.HDFStore('try_hyerar.h5','a')\n",
    "\n",
    "store.append('city/topic', df_try2)\n",
    "\n",
    "store.close()\n",
    "\n",
    "pd.read_hdf('try_hyerar.h5', 'city/topic')\n",
    "\n",
    "store = pd.HDFStore('try_hyerar.h5','a')\n",
    "\n",
    "store.put('city/followers',df)\n",
    "\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MY ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ca'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = tweepy.API(auth)\n",
    "user_info = api.get_user('ArnauAndreu')  \n",
    "user_info._json['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ukraine'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info._json['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_friends = get_account_network('ArnauAndreu')\n",
    "\n",
    "my_df = pd.DataFrame(my_followers)\n",
    "\n",
    "Counter([friend.lang for friend in my_friends])\n",
    "\n",
    "my_fr_txts, my_friends_lang = get_twts_from_list_account_names(my_friends)\n",
    "\n",
    "#Counter(my_friends_lang)\n",
    "\n",
    "\n",
    "\n",
    "# my_fr_langs_detected=[]\n",
    "# for i,txt in enumerate(my_fr_txts):\n",
    "#     #print(i, txt)\n",
    "#     try:\n",
    "#         my_fr_langs_detected.append(detect(txt))\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "#Counter(my_fr_langs_detected)\n",
    "\n",
    "usr_tl = get_account_timeline(my_friends[44].screen_name, max_num=10)\n",
    "\n",
    "df_try=pd.DataFrame([twt._json for twt in usr_tl])\n",
    "df_try.columns\n",
    "\n",
    "my_tl = get_account_timeline('ArnauAndreu', max_num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DYNAMO KIEV PLAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DYNAMO PLAYERS\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"http://www.worldfootball.net/teams/dinamo-kiev/2017/2/\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, \"lxml\")\n",
    "\n",
    "dynamo_players = pd.read_html(url, encoding='utf8')[1][2].dropna().values\n",
    "\n",
    "dynamo_players\n",
    "\n",
    "dyn_play_countr = pd.read_html(url, encoding='utf8')[1][[2,4]].dropna().values\n",
    "\n",
    "dyn_play_countr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYMONGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "conn=MongoClient()\n",
    "\n",
    "#define database\n",
    "db = conn.citylangs\n",
    "\n",
    "#define collection inside database\n",
    "collection = db.bcn.ajuntam.followers\n",
    "\n",
    "# function to add documents to collection \n",
    "def make_followers_collection(account_name, collection, max_num=100):\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    users = tweepy.Cursor(api.followers, screen_name=account_name).items(max_num)\n",
    "    #i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            user = next(users)\n",
    "            collection.insert_one(user._json)\n",
    "        except tweepy.TweepError as e:\n",
    "            if 'Read timed out' in str(e):\n",
    "                print('fall here')\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                time.sleep(60*16)\n",
    "                user = next(users)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        #print (\"@\" + user.screen_name)\n",
    "    #return collection\n",
    "\n",
    "make_followers_collection('bcn_ajuntament',collection, max_num=100)\n",
    "\n",
    "#check what databases are available\n",
    "conn.database_names()\n",
    "\n",
    "# available collections inside db\n",
    "db.collection_names()\n",
    "\n",
    "collection2 = db.kiev.ukrpravda\n",
    "\n",
    "make_followers_collection('ukrpravda_news',collection2, max_num=300)\n",
    "\n",
    "l = list(conn.citylangs.kiev.ukrpravda.find())\n",
    "\n",
    "Counter([obj['lang'] for obj in l])\n",
    "\n",
    "\n",
    "db.collection_names()\n",
    "\n",
    "#db.categories.insert_one({ \"_id\": \"ukr_pravda\", \"children\": [] })\n",
    "# db.categories.insert({ _id: \"avto_kiev\", children: [] })\n",
    "# db.categories.insert({ _id: \"kiev\", children: [\"ukr_pravda\", \"avto_kiev\"] })\n",
    "\n",
    "db['kiev'].insert_one({'avto_kiev':[],'vitklitschko':[]})\n",
    "\n",
    "db.collection_names()\n",
    "\n",
    "rr=list(db['bcn.ajuntam.followers'].find())\n",
    "\n",
    "db['kiev'].find_one()\n",
    "\n",
    "db['bcn.ajuntam.followers']\n",
    "\n",
    "conn.database_names()\n",
    "\n",
    "coll2 = db.countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get hdf database keys\n",
    "with pd.HDFStore('lang_data.h5','r') as f:\n",
    "    my_keys = f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/ukr_nodes/news/BBC_ua/followers',\n",
       " '/ukr_nodes/news/BBC_ua/tls_followers',\n",
       " '/ukr_nodes/news/HromadskeUA/followers',\n",
       " '/ukr_nodes/news/HromadskeUA/tls_followers',\n",
       " '/ukr_nodes/news/LIGAnet/followers',\n",
       " '/ukr_nodes/news/LIGAnet/tls_followers',\n",
       " '/ukr_nodes/news/ukrpravda_news/followers',\n",
       " '/cat_nodes/news/LaVanguardia/followers',\n",
       " '/cat_nodes/news/LaVanguardia/tls_followers',\n",
       " '/cat_nodes/news/diariARA/followers',\n",
       " '/cat_nodes/news/diariARA/tls_followers']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'lang_data.h5'\n",
    "country = 'cat'\n",
    "acc_names = ['diariARA', 'LaVanguardia']\n",
    "load_node1 = '/' + country + '_nodes/news/' + acc_names[0] + '/tls_followers'\n",
    "load_node2 = '/' + country + '_nodes/news/' + acc_names[1] + '/tls_followers'\n",
    "\n",
    "df1 = pd.read_hdf(file_path, load_node1)\n",
    "df2 = pd.read_hdf(file_path, load_node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df1, df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44937, 2), (34461, 2), (12181, 2))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape, df1.shape, df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12a432550>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fb_token = 'EAADT8MeSHbsBAPi29inYYj1xZBsywhwduhm5XZA1jI07qzmPA7OLVPZBBtRDCPv7LTyUvhxWut5ztyZAS5lvNMUU'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
